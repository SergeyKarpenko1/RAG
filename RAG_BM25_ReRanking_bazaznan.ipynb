{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "import os\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from getpass import getpass\n",
    "\n",
    "openai.api_key = getpass(\"Please provide your OpenAI Key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка максимального количества выводимых строк в DataFrame\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка и чтение файлов в txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружено 1 документов.\n",
      "\n",
      "--- Используем рекурсивное разбиение на чанки ---\n",
      "Разбито на 360 чанков.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Получаем текущий рабочий каталог\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Определяем путь к файлу\n",
    "file_path = os.path.join(current_dir, \"/Users/sergey/Desktop/RAG_Project_FULL/RAG_intelion/Intelion_books/bazaznanii.txt\")\n",
    "\n",
    "# Определяем путь к директории для хранилища\n",
    "persistent_directory = os.path.join(current_dir, \"db\", \"chroma_db_bazaznanii\")\n",
    "\n",
    "\n",
    "db_dir = os.path.join(current_dir, \"db\")\n",
    "\n",
    "# Проверка, существует ли текстовый файл, с которого нужно загружать данные\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"Файл {file_path} не найден. Проверьте путь.\")\n",
    "else:\n",
    "    # Если файл найден, загружаем его содержимое с помощью загрузчика текстовых данных\n",
    "    loader = TextLoader(file_path)\n",
    "    documents = loader.load()  # Загружаем документы из текстового файла\n",
    "    print(f\"Загружено {len(documents)} документов.\")\n",
    "\n",
    "# Если документы загружены, используем рекурсивный сплиттер для разбиения\n",
    "print(\"\\n--- Используем рекурсивное разбиение на чанки ---\")\n",
    "Recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150, chunk_overlap=50\n",
    ")\n",
    "\n",
    "# Разбиваем документы на чанки\n",
    "docs = Recursive_text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Разбито на {len(docs)} чанков.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Чтение и разбиение файлов на чанки при помощи RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для создания и сохранения векторного хранилища\n",
    "def create_vector_store(docs, store_name):\n",
    "    \n",
    "    # Определение пути к директории, где будет сохранено векторное хранилище\n",
    "    persistent_directory = os.path.join(db_dir, store_name)\n",
    "    \n",
    "    # Проверка, существует ли уже директория для хранения векторного хранилища\n",
    "    if not os.path.exists(persistent_directory):\n",
    "        \n",
    "        # Если директория не существует, выводим сообщение о создании векторного хранилища\n",
    "        print(f\"\\n--- Создание векторного хранилища {store_name} ---\")\n",
    "        \n",
    "        # Создаем векторное хранилище из документов с использованием заданной модели embeddings\n",
    "        # и сохраняем его в указанную директорию\n",
    "        db = Chroma.from_documents(\n",
    "            docs, embeddings, persist_directory=persistent_directory\n",
    "        )\n",
    "        \n",
    "        # Сообщаем, что создание векторного хранилища завершено\n",
    "        print(f\"--- Создание векторного хранилища завершено {store_name} ---\")\n",
    "    \n",
    "    # Если директория уже существует, выводим сообщение, что хранилище уже создано, и инициализация не требуется\n",
    "    else:\n",
    "        print(\n",
    "            f\"Хранилище {store_name} хранилище уже создано, и инициализация не требуется.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# В качестве эмбеддингов используем text-embedding-3-small от OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем модель эмбеддингов\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h2/r9wh0x750xq4v5z33ylwrzw40000gn/T/ipykernel_75009/923028654.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
      "/Users/sergey/Desktop/RAG_Project_FULL/.venv/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"intfloat/multilingual-e5-base\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создание векторной базы данных (используем разные эмбеддинги)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_vector_store(docs, \"chroma_db_intelion_BazaZnanii_OAI_emb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_vector_store(docs, \"chroma_db_intelion_BazaZnanii_e5_emb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка векторной базы данных после ее создания или если она была создана ранее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружаем векторное хранилище из /Users/sergey/Desktop/RAG_Project_FULL/RAG_intelion/db/chroma_db_intelion_BazaZnanii_OAI_emb...\n"
     ]
    }
   ],
   "source": [
    "# Укажите путь к вашей базе данных\n",
    "persistent_directory = \"/Users/sergey/Desktop/RAG_Project_FULL/RAG_intelion/db/chroma_db_intelion_BazaZnanii_OAI_emb\"\n",
    "\n",
    "# Проверьте, существует ли база данных\n",
    "if os.path.exists(os.path.join(persistent_directory, \"chroma.sqlite3\")):\n",
    "    print(f\"Загружаем векторное хранилище из {persistent_directory}...\")\n",
    "\n",
    "    # Загружаем векторное хранилище\n",
    "    db = Chroma(\n",
    "        persist_directory=persistent_directory,  # Указываем путь к существующему хранилищу\n",
    "        embedding_function=embeddings  # Передаем функцию эмбеддингов (нужна для поиска)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Этот код создаёт объект BM25Retriever, который будет использовать алгоритм BM25 (Best Matching 25) для извлечения релевантных документов из набора данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rank_bm25\n",
    "# Создание объекта BM25Retriever на основе переданных документов (docs).\n",
    "# BM25 — это алгоритм для поиска релевантных документов на основе частоты ключевых слов в документе.\n",
    "keyword_retriever = BM25Retriever.from_documents(docs)\n",
    "\n",
    "# Установка количества документов, которые будут возвращаться при каждом поисковом запросе, на 5.\n",
    "# Это ограничение определяет, что максимум 5 документов будут считаться наиболее релевантными результатами.\n",
    "keyword_retriever.k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование объекта db (который представляет векторную базу данных) в ретривер.\n",
    "# В данном случае используется тип поиска \"similarity\", что означает, что поиск будет основан на схожести (например, косинусное расстояние).\n",
    "retriever_similarity = db.as_retriever(\n",
    "    \n",
    "    # Установка типа поиска \"similarity\". Это указывает на то, что будет использоваться поиск по сходству между векторными представлениями данных.\n",
    "    search_type=\"similarity\",\n",
    "    \n",
    "    # Дополнительные параметры для поиска:\n",
    "    # \"k\": 10 — ограничение на количество возвращаемых результатов. Будет возвращено до 10 самых похожих документов.\n",
    "    search_kwargs={\"k\": 10},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## объект EnsembleRetriever, который объединяет несколько поисковых систем (ретриверов) для извлечения информации.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание объекта EnsembleRetriever, который объединяет несколько ретриверов.\n",
    "# 'db' и 'keyword_retriever' - это два ретривера, которые будут использованы для поиска.\n",
    "ensemble_retriever = EnsembleRetriever(retrievers=[retriever_similarity, \n",
    "                                                   keyword_retriever],\n",
    "                                       # Весовые коэффициенты для каждого ретривера.\n",
    "                                       # Оба ретривера имеют равные веса по 0.5, то есть их вклад в результат одинаков.\n",
    "                                       weights=[0.6, 0.4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проверяем как работает поиск по базе данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI эмбеддинги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем вопрос, на который будем искать ответ\n",
    "query = \"Кто ген дир?\"\n",
    "\n",
    "# Выполняем поиск с использованием EnsembleRetriever\n",
    "relevant_docs_ensemble = ensemble_retriever.invoke(query)\n",
    "\n",
    "# Вывод результатов для EnsembleRetriever\n",
    "print(\"\\n--- Результаты поиска с использованием EnsembleRetriever ---\")\n",
    "for i, doc in enumerate(relevant_docs_ensemble, 1):\n",
    "    print(f\"Документ {i}:\\n{doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# e5 эмбеддинги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Результаты поиска с использованием EnsembleRetriever ---\n",
      "Документ 1:\n",
      "Директор по маркетингу: Денис Губанов\n",
      "\n",
      "Документ 2:\n",
      "АР – Александр Александрович Рощин, Операционный директор\n",
      "\n",
      "Документ 3:\n",
      "Направление Дизайн\n",
      "Руководитель Даниил Беленок\n",
      "\n",
      "Документ 4:\n",
      "ТА – Тимофей Андреевич Семенов, Генеральный директор\n",
      "\n",
      "Документ 5:\n",
      "изменения с руководителем. Если он сам является руководителем подразделения, то обращаться нужно к\n",
      "\n",
      "Документ 6:\n",
      "МГ – Максим Геннадьевич Симуткин, Директор по развитию, энергетике и строительству\n",
      "\n",
      "Документ 7:\n",
      "отдела, а руководитель отдела — со своим руководителей и HRD.\n",
      "\n",
      "Документ 8:\n",
      "Направление SEO\n",
      "Руководитель Глеб Крячок\n",
      "\n",
      "Документ 9:\n",
      "О компании\n",
      "\n",
      "Документ 10:\n",
      "Направление Разработка\n",
      "Руководитель Криштун Роман\n",
      "\n",
      "Документ 11:\n",
      "[b]Внутри файл https://intelionmining.bitrix24.ru/knowledge/howintelionworks/filosofiyaintelion_hruw/\n",
      "\n",
      "Документ 12:\n",
      "Для строительства нового ЦОДа Intelion обязательно выбираются регионы с низкой стоимостью электроэнергии и свободным объемом электрической мощности\n",
      "\n",
      "Документ 13:\n",
      "с крупной генерацией, которая может обеспечить покрытие дополнительного спроса на электроэнергию.\n",
      "\n",
      "Документ 14:\n",
      "В своих технических решениях компания отдает предпочтение мобильным и модульным решениям (размещение оборудования в специализированных морских\n",
      "\n",
      "Документ 15:\n",
      "оборудования в специализированных морских контейнерах), которые обеспечивают быстрое строительство и запуск площадки.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Определяем вопрос, на который будем искать ответ\n",
    "query = \"Кто ген дир?\"\n",
    "\n",
    "# Создание объекта EnsembleRetriever, который объединяет несколько ретриверов:\n",
    "# 'retriever_similarity' и 'keyword_retriever' - два ретривера, участвующие в ансамбле.\n",
    "ensemble_retriever = EnsembleRetriever(retrievers=[retriever_similarity, \n",
    "                                                   keyword_retriever],\n",
    "                                       # Весовые коэффициенты для каждого ретривера.\n",
    "                                       # Оба ретривера имеют равные веса по 0.5, что означает, что их вклад в общий результат равнозначен.\n",
    "                                       weights=[0.6, 0.4])\n",
    "\n",
    "# Выполняем поиск с использованием EnsembleRetriever\n",
    "relevant_docs_ensemble = ensemble_retriever.invoke(query)\n",
    "\n",
    "# Вывод результатов для EnsembleRetriever\n",
    "print(\"\\n--- Результаты поиска с использованием EnsembleRetriever ---\")\n",
    "for i, doc in enumerate(relevant_docs_ensemble, 1):\n",
    "    print(f\"Документ {i}:\\n{doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Самый простой вариант\n",
    "## здесь добовляем генерацию ответа LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Просто поиск по векторной базе "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Релевантные документы ---\n",
      "Документ 1:\n",
      "Директор по маркетингу: Денис Губанов\n",
      "\n",
      "Документ 2:\n",
      "АР – Александр Александрович Рощин, Операционный директор\n",
      "\n",
      "Документ 3:\n",
      "Направление Дизайн\n",
      "Руководитель Даниил Беленок\n",
      "\n",
      "Документ 4:\n",
      "ТА – Тимофей Андреевич Семенов, Генеральный директор\n",
      "\n",
      "Документ 5:\n",
      "изменения с руководителем. Если он сам является руководителем подразделения, то обращаться нужно к\n",
      "\n",
      "Документ 6:\n",
      "МГ – Максим Геннадьевич Симуткин, Директор по развитию, энергетике и строительству\n",
      "\n",
      "Документ 7:\n",
      "отдела, а руководитель отдела — со своим руководителей и HRD.\n",
      "\n",
      "Документ 8:\n",
      "Направление SEO\n",
      "Руководитель Глеб Крячок\n",
      "\n",
      "Документ 9:\n",
      "Организационные вопросы\n",
      "\n",
      "Документ 10:\n",
      "О компании\n",
      "\n",
      "Документ 11:\n",
      "Направление Разработка\n",
      "Руководитель Криштун Роман\n",
      "\n",
      "Документ 12:\n",
      "Операционный директор Александр Рощин\n",
      "\n",
      "Документ 13:\n",
      "ЮИ – Юлия Игоревна Яровова, Директор по правовым вопросам\n",
      "\n",
      "Документ 14:\n",
      "Юридический отдел\n",
      "Директор по правовым вопросам Юлия Игоревна Яровова\n",
      "\n",
      "Документ 15:\n",
      "их руководителю своего отдела, HRD Анастасии (@bessonastya) или Старшему HR Яне (@Yana99s). Они\n",
      "\n",
      "\n",
      "--- Сгенерированный ответ ---\n",
      "Текстовый ответ модели:\n",
      "Генеральный директор (ген дир) — Тимофей Андреевич Семенов.\n"
     ]
    }
   ],
   "source": [
    "# Определяем вопрос, на который будем искать ответ\n",
    "query = \"Кто ген дир?\"\n",
    "\n",
    "# 1. Поиск по сходству без порога (search_type=\"similarity\")\n",
    "retriever_similarity = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 15},\n",
    ")\n",
    "\n",
    "# Выполняем запрос к ансамблю ретриверов для получения релевантных документов\n",
    "relevant_docs = retriever_similarity.invoke(query)\n",
    "\n",
    "# Выводим релевантные документы с их содержимым\n",
    "print(\"\\n--- Релевантные документы ---\")\n",
    "for i, doc in enumerate(relevant_docs, 1):\n",
    "    # Выводим каждый документ с его содержимым и номером\n",
    "    print(f\"Документ {i}:\\n{doc.page_content}\\n\")\n",
    "\n",
    "# Объединяем запрос и содержимое релевантных документов в один текст для передачи модели\n",
    "combined_input = (\n",
    "    \"Это некоторые документы которые могут помочь ответить на вопрос: \"\n",
    "    + query  # Включаем исходный запрос\n",
    "    + \"\\n\\nРелевантные документы:\\n\"  # Добавляем заголовок для раздела с документами\n",
    "    + \"\\n\\n\".join([doc.page_content for doc in relevant_docs])  # Добавляем текст всех документов\n",
    "    + \"\\n\\nПожалуйста, предоставьте ответ, основываясь только на предоставленных документах. Если ответ не найден в документах, ответьте ‘Я не уверен'.\"  # Просим модель ответить только на основе предоставленных данных\n",
    ")\n",
    "\n",
    "# Создаем модель ChatOpenAI для обработки запроса\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Определяем сообщения, которые передаем модели\n",
    "messages = [\n",
    "    SystemMessage(content=\"Ты помощник, который отвечает на вопросы пользователя.\"),  # Системное сообщение с инструкциями для модели\n",
    "    HumanMessage(content=combined_input),  # Сообщение пользователя, содержащее запрос и документы\n",
    "]\n",
    "\n",
    "# Вызываем модель с подготовленными сообщениями\n",
    "result = model.invoke(messages)\n",
    "\n",
    "# Выводим сгенерированный ответ\n",
    "print(\"\\n--- Сгенерированный ответ ---\")\n",
    "print(\"Текстовый ответ модели:\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Поиск по обьедененным векторной базе данных и поиск по ключевым словам "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Релевантные документы ---\n",
      "Документ 1:\n",
      "Директор по маркетингу: Денис Губанов\n",
      "\n",
      "Документ 2:\n",
      "АР – Александр Александрович Рощин, Операционный директор\n",
      "\n",
      "Документ 3:\n",
      "Направление Дизайн\n",
      "Руководитель Даниил Беленок\n",
      "\n",
      "Документ 4:\n",
      "ТА – Тимофей Андреевич Семенов, Генеральный директор\n",
      "\n",
      "Документ 5:\n",
      "изменения с руководителем. Если он сам является руководителем подразделения, то обращаться нужно к\n",
      "\n",
      "Документ 6:\n",
      "МГ – Максим Геннадьевич Симуткин, Директор по развитию, энергетике и строительству\n",
      "\n",
      "Документ 7:\n",
      "отдела, а руководитель отдела — со своим руководителей и HRD.\n",
      "\n",
      "Документ 8:\n",
      "Направление SEO\n",
      "Руководитель Глеб Крячок\n",
      "\n",
      "Документ 9:\n",
      "О компании\n",
      "\n",
      "Документ 10:\n",
      "Направление Разработка\n",
      "Руководитель Криштун Роман\n",
      "\n",
      "Документ 11:\n",
      "[b]Внутри файл https://intelionmining.bitrix24.ru/knowledge/howintelionworks/filosofiyaintelion_hruw/\n",
      "\n",
      "Документ 12:\n",
      "Для строительства нового ЦОДа Intelion обязательно выбираются регионы с низкой стоимостью электроэнергии и свободным объемом электрической мощности\n",
      "\n",
      "Документ 13:\n",
      "с крупной генерацией, которая может обеспечить покрытие дополнительного спроса на электроэнергию.\n",
      "\n",
      "Документ 14:\n",
      "В своих технических решениях компания отдает предпочтение мобильным и модульным решениям (размещение оборудования в специализированных морских\n",
      "\n",
      "Документ 15:\n",
      "оборудования в специализированных морских контейнерах), которые обеспечивают быстрое строительство и запуск площадки.\n",
      "\n",
      "\n",
      "--- Сгенерированный ответ ---\n",
      "Текстовый ответ модели:\n",
      "Генеральный директор (ген дир) - Тимофей Андреевич Семенов.\n"
     ]
    }
   ],
   "source": [
    "# Определяем вопрос, на который будем искать ответ\n",
    "query = \"Кто ген дир?\"\n",
    "\n",
    "# Выполняем запрос к ансамблю ретриверов для получения релевантных документов\n",
    "relevant_docs = ensemble_retriever.invoke(query)\n",
    "\n",
    "# Выводим релевантные документы с их содержимым\n",
    "print(\"\\n--- Релевантные документы ---\")\n",
    "for i, doc in enumerate(relevant_docs, 1):\n",
    "    # Выводим каждый документ с его содержимым и номером\n",
    "    print(f\"Документ {i}:\\n{doc.page_content}\\n\")\n",
    "\n",
    "# Объединяем запрос и содержимое релевантных документов в один текст для передачи модели\n",
    "combined_input = (\n",
    "    \"Это некоторые документы которые могут помочь ответить на вопрос: \"\n",
    "    + query  # Включаем исходный запрос\n",
    "    + \"\\n\\nРелевантные документы:\\n\"  # Добавляем заголовок для раздела с документами\n",
    "    + \"\\n\\n\".join([doc.page_content for doc in relevant_docs])  # Добавляем текст всех документов\n",
    "    + \"\\n\\nПожалуйста, предоставьте ответ, основываясь только на предоставленных документах. Если ответ не найден в документах, ответьте ‘Я не уверен'.\"  # Просим модель ответить только на основе предоставленных данных\n",
    ")\n",
    "\n",
    "# Создаем модель ChatOpenAI для обработки запроса\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Определяем сообщения, которые передаем модели\n",
    "messages = [\n",
    "    SystemMessage(content=\"Ты помощник, который отвечает на вопросы пользователя.\"),  # Системное сообщение с инструкциями для модели\n",
    "    HumanMessage(content=combined_input),  # Сообщение пользователя, содержащее запрос и документы\n",
    "]\n",
    "\n",
    "# Вызываем модель с подготовленными сообщениями\n",
    "result = model.invoke(messages)\n",
    "\n",
    "# Выводим сгенерированный ответ\n",
    "print(\"\\n--- Сгенерированный ответ ---\")\n",
    "print(\"Текстовый ответ модели:\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 2. Вариант с перефразированием и использованием контекста всей беседы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вариант с выводом перефразированных вопросов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Только векторная база данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начните общение с ИИ! Введите ‘выход’, чтобы завершить разговор.\n",
      "Вы: Кто ген дир?\n",
      "Переформулированный вопрос: Кто является генеральным директором?\n",
      "AI: Генеральным директором является Тимофей Андреевич Семенов.\n"
     ]
    }
   ],
   "source": [
    "# Создаем модель ChatOpenAI с использованием модели GPT-4o\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")  # Модель GPT-4o для обработки запросов\n",
    "\n",
    "# Определяем системный промпт для контекстуализации вопросов\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Учитывая историю чата и последний вопрос пользователя, \"\n",
    "    \"который может ссылаться на контекст из истории чата, \"\n",
    "    \"сформулируйте самодостаточный вопрос, который можно понять \"\n",
    "    \"без использования истории чата. НЕ отвечайте на вопрос, просто \"\n",
    "    \"переформулируйте его, если это необходимо, или верните его без изменений.\"\n",
    ")\n",
    "\n",
    "# Создаем шаблон для формирования контекстуализированных вопросов\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Создаем retriever, учитывающий историю чата\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever_similarity, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "# Определяем системный промпт для ответа на вопросы\n",
    "qa_system_prompt = (\n",
    "    \"Вы помощник для задач по ответам на вопросы из справочника конструктора машиностроителя. \"\n",
    "    \"Используйте следующие части полученного контекста, чтобы ответить на вопрос. \"\n",
    "    \"Используйте только контекст, полученный из базы данных, чтобы ответить на вопрос. \"\n",
    "    \"Если контекст не найден или не подходит для ответа, скажите, что вы не знаете ответа. \"\n",
    "    \"Используйте максимум три предложения. Ответ должен быть кратким.\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "# Создаем шаблон для формирования ответов на вопросы\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Создаем цепочку для объединения документов для ответов на вопросы\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "# Создаем retrieval-цепочку, которая объединяет retriever с историей и цепочку для ответов на вопросы\n",
    "simple_vector_store_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "# Функция для моделирования непрерывного чата с выводом переформулированного вопроса и релевантных документов из базы данных\n",
    "def continual_chat():\n",
    "    print(\"Начните общение с ИИ! Введите ‘выход’, чтобы завершить разговор.\")  # Начало общения с ИИ\n",
    "    chat_history = []  # Хранение истории чата\n",
    "    while True:\n",
    "        query = input(\"ВЫ: \")  # Получение запроса от пользователя\n",
    "        if query.lower() == \"выход\":  # Если пользователь хочет завершить чат\n",
    "            break\n",
    "\n",
    "        # Переформулировка вопроса через вызов LLM с контекстом\n",
    "        reformulated_prompt = contextualize_q_prompt.format_messages(\n",
    "            chat_history=chat_history, input=query\n",
    "        )\n",
    "        reformulated_query_result = llm(reformulated_prompt)  # Получаем объект AIMessage\n",
    "        reformulated_query = reformulated_query_result.content  # Доступ к содержимому через атрибут content\n",
    "\n",
    "        # Получение релевантных документов напрямую из базы данных\n",
    "        relevant_docs = ensemble_retriever.get_relevant_documents(reformulated_query)\n",
    "\n",
    "        # Печать исходного вопроса, переформулированного вопроса и релевантных документов с метриками\n",
    "        print(f\"Вы: {query}\")  # Печать исходного вопроса пользователя\n",
    "        print(f\"Переформулированный вопрос: {reformulated_query}\")  # Вывод переформулированного вопроса\n",
    "\n",
    "        # Обработка запроса пользователя через цепочку retrieval для получения финального ответа\n",
    "        retrieval_result = simple_vector_store_chain.invoke({\"input\": reformulated_query, \"chat_history\": chat_history})  # Запрос к цепочке\n",
    "\n",
    "        # Печать ответа ИИ\n",
    "        print(f\"AI: {retrieval_result['answer']}\")  # Ответ ИИ\n",
    "\n",
    "        # Обновление истории чата\n",
    "        chat_history.append(HumanMessage(content=query))  # Добавление сообщения пользователя в историю\n",
    "        chat_history.append(SystemMessage(content=retrieval_result[\"answer\"]))  # Добавление ответа ИИ в историю\n",
    "\n",
    "# Main function to start the continual chat\n",
    "if __name__ == \"__main__\":\n",
    "    continual_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Поиск по векторной базе и ключевым словам "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начните общение с ИИ! Введите ‘выход’, чтобы завершить разговор.\n",
      "Вы: Кто ген дир?\n",
      "Переформулированный вопрос: Кто является генеральным директором?\n",
      "AI: Генеральным директором является Тимофей Андреевич Семенов.\n"
     ]
    }
   ],
   "source": [
    "# Создаем модель ChatOpenAI с использованием модели GPT-4o\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")  # Модель GPT-4o для обработки запросов\n",
    "\n",
    "# Определяем системный промпт для контекстуализации вопросов\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Учитывая историю чата и последний вопрос пользователя, \"\n",
    "    \"который может ссылаться на контекст из истории чата, \"\n",
    "    \"сформулируйте самодостаточный вопрос, который можно понять \"\n",
    "    \"без использования истории чата. НЕ отвечайте на вопрос, просто \"\n",
    "    \"переформулируйте его, если это необходимо, или верните его без изменений.\"\n",
    ")\n",
    "\n",
    "# Создаем шаблон для формирования контекстуализированных вопросов\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),  # Системное сообщение с инструкцией для ИИ\n",
    "        MessagesPlaceholder(\"chat_history\"),  # Место для истории чата\n",
    "        (\"human\", \"{input}\"),  # Ввод пользователя, который нужно переформулировать\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Создаем retriever, учитывающий историю чата\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, ensemble_retriever, contextualize_q_prompt  # Используем созданный шаблон и retriever для поиска с историей\n",
    ")\n",
    "\n",
    "# Определяем системный промпт для ответа на вопросы\n",
    "qa_system_prompt = (\n",
    "    \"Вы помощник для задач по ответам на вопросы из справочника конструктора машиностроителя. \"\n",
    "    \"Используйте следующие части полученного контекста, чтобы ответить на вопрос. \"\n",
    "    \"Используйте только контекст, полученный из базы данных, чтобы ответить на вопрос. \"\n",
    "    \"Если контекст не найден или не подходит для ответа, скажите, что вы не знаете ответа. \"\n",
    "    \"Используйте максимум три предложения. Ответ должен быть кратким.\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "# Создаем шаблон для формирования ответов на вопросы\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),  # Системное сообщение с инструкцией для ИИ\n",
    "        MessagesPlaceholder(\"chat_history\"),  # Место для истории чата\n",
    "        (\"human\", \"{input}\"),  # Ввод пользователя, на который нужно ответить\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Создаем цепочку для объединения документов для ответов на вопросы\n",
    "# `create_stuff_documents_chain` передает весь найденный контекст в LLM для обработки\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "# Создаем retrieval-цепочку, которая объединяет retriever с историей и цепочку для ответов на вопросы\n",
    "simple_vector_store_and_BM25 = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "# Функция для моделирования непрерывного чата с выводом переформулированного вопроса и релевантных документов из базы данных\n",
    "def continual_chat():\n",
    "    print(\"Начните общение с ИИ! Введите ‘выход’, чтобы завершить разговор.\")  # Начало общения с ИИ\n",
    "    chat_history = []  # Хранение истории чата\n",
    "    while True:\n",
    "        query = input(\"ВЫ: \")  # Получение запроса от пользователя\n",
    "        if query.lower() == \"выход\":  # Если пользователь хочет завершить чат\n",
    "            break\n",
    "\n",
    "        # Переформулировка вопроса через вызов LLM с контекстом\n",
    "        reformulated_prompt = contextualize_q_prompt.format_messages(\n",
    "            chat_history=chat_history, input=query\n",
    "        )\n",
    "        reformulated_query_result = llm(reformulated_prompt)  # Получаем объект AIMessage\n",
    "        reformulated_query = reformulated_query_result.content  # Доступ к содержимому через атрибут content\n",
    "\n",
    "        # Получение релевантных документов напрямую из базы данных\n",
    "        relevant_docs = ensemble_retriever.get_relevant_documents(reformulated_query)\n",
    "\n",
    "        # Печать исходного вопроса, переформулированного вопроса и релевантных документов с метриками\n",
    "        print(f\"Вы: {query}\")  # Печать исходного вопроса пользователя\n",
    "        print(f\"Переформулированный вопрос: {reformulated_query}\")  # Вывод переформулированного вопроса\n",
    "\n",
    "        # Обработка запроса пользователя через цепочку retrieval для получения финального ответа\n",
    "        retrieval_result = simple_vector_store_and_BM25.invoke({\"input\": reformulated_query, \"chat_history\": chat_history})  # Запрос к цепочке\n",
    "\n",
    "        # Печать ответа ИИ\n",
    "        print(f\"AI: {retrieval_result['answer']}\")  # Ответ ИИ\n",
    "\n",
    "        # Обновление истории чата\n",
    "        chat_history.append(HumanMessage(content=query))  # Добавление сообщения пользователя в историю\n",
    "        chat_history.append(SystemMessage(content=retrieval_result[\"answer\"]))  # Добавление ответа ИИ в историю\n",
    "\n",
    "# Main function to start the continual chat\n",
    "if __name__ == \"__main__\":\n",
    "    continual_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## То же самое что и выше только с использованием переранжирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начните общение с ИИ! Введите ‘выход’, чтобы завершить разговор.\n",
      "Вы: Кто ген дир?\n",
      "AI: Генеральный директор компании Intelion — Тимофей Андреевич Семенов.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_transformers import LongContextReorder\n",
    "\n",
    "# Создаем модель ChatOpenAI с использованием модели GPT-4o\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)  # Модель GPT-4o для обработки запросов\n",
    "\n",
    "# Определяем системный промпт для контекстуализации вопросов\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Учитывая историю чата и последний вопрос пользователя, \"\n",
    "    \"который может ссылаться на контекст из истории чата, \"\n",
    "    \"сформулируйте самодостаточный вопрос, который можно понять \"\n",
    "    \"без использования истории чата. НЕ отвечайте на вопрос, просто \"\n",
    "    \"переформулируйте его, если это необходимо, или верните его без изменений.\"\n",
    ")\n",
    "\n",
    "# Создаем шаблон для формирования контекстуализированных вопросов\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Создаем retriever, учитывающий историю чата\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, ensemble_retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "# Определяем системный промпт для ответа на вопросы\n",
    "qa_system_prompt = (\n",
    "    \"Вы помощник для задач по ответам на вопросы о компании Intelion. \"\n",
    "    \"Используйте следующие части полученного контекста, чтобы ответить на вопрос. \"\n",
    "    \"Если вы не знаете ответа, просто скажите, что вы не знаете. \"\n",
    "    \"Используйте максимум три предложения и дайте краткий ответ.\"\n",
    "    \"\\n\\n{context}\"\n",
    ")\n",
    "\n",
    "# Создаем шаблон для формирования ответов на вопросы\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Создаем цепочку для объединения документов для ответов на вопросы\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "# Создаем объект LongContextReorder для переранжирования длинных контекстов\n",
    "long_context_reorder = LongContextReorder()\n",
    "\n",
    "# Создаем базовую retrieval-цепочку без переранжирования\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "# Создаем цепочку с переранжированием для документов — назовем её simple_reranked\n",
    "def simple_reranked_chain(input_query, chat_history):\n",
    "    # Получение результата от базовой retrieval-цепочки\n",
    "    result = rag_chain.invoke({\"input\": input_query, \"chat_history\": chat_history})\n",
    "\n",
    "    # Применение LongContextReorder для переранжирования длинных документов\n",
    "    reordered_docs = long_context_reorder.transform_documents(result[\"context\"])\n",
    "\n",
    "    # Возвращаем переранжированные документы и ответ ИИ\n",
    "    return {\"answer\": result[\"answer\"], \"reordered_docs\": reordered_docs}\n",
    "\n",
    "# Функция для моделирования непрерывного чата с использованием simple_reranked_chain\n",
    "def continual_chat():\n",
    "    print(\"Начните общение с ИИ! Введите ‘выход’, чтобы завершить разговор.\")\n",
    "    chat_history = []  # Хранение истории чата\n",
    "    while True:\n",
    "        query = input(\"ВЫ: \")  # Получение запроса от пользователя\n",
    "        if query.lower() == \"выход\":  # Если пользователь хочет завершить чат\n",
    "            break\n",
    "\n",
    "        # Использование цепочки с переранжированием\n",
    "        result = \n",
    "        (query, chat_history)\n",
    "        \n",
    "        # Печать вопроса пользователя и ответа ИИ\n",
    "        print(f\"Вы: {query}\")\n",
    "        print(f\"AI: {result['answer']}\")  # Ответ ИИ\n",
    "        # print(f\"Переранжированные документы: {result['reordered_docs']}\")  # Переранжированные документы\n",
    "        \n",
    "        # Обновление истории чата\n",
    "        chat_history.append(HumanMessage(content=query))  # Добавление сообщения пользователя в историю\n",
    "        chat_history.append(SystemMessage(content=result[\"answer\"]))  # Добавление ответа ИИ в историю\n",
    "\n",
    "# Main function to start the continual chat\n",
    "if __name__ == \"__main__\":\n",
    "    continual_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LongContextReorder выполняет переранжирование документов, чтобы минимизировать эффект “затерянности посередине”. Описание работы:\n",
    "\n",
    "\t1.\tИсходное ранжирование:\n",
    "\t•\tДокументы от retriever'а поступают в порядке убывания релевантности (наиболее релевантные документы в начале списка).\n",
    "\t2.\tАлгоритм изменения порядка:\n",
    "\t•\tLongContextReorder перераспределяет документы так, чтобы наиболее релевантные документы оказались в начале и в конце списка, а менее релевантные документы — в середине.\n",
    "\n",
    "\tКонкретные шаги:\n",
    "\t•\tБерется первый и последний документ из исходного списка (самые релевантные) и размещаются на крайних позициях (первый и последний).\n",
    "\t•\tСледующий по релевантности документ размещается во вторую позицию, предпоследний — на вторую с конца, и так далее.\n",
    "\t•\tТаким образом, релевантные документы занимают ключевые позиции (начало и конец), что помогает модели сфокусироваться на важных данных в начале и конце контекста, а не пропускать \tих \tв середине.\n",
    "\t3.\tРезультат:\n",
    "\t•\tВыводится упорядоченный список документов, в котором релевантные документы на краях контекста, что может повысить качество ответов языковой модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Поиск документов с использованием RAG-Fusion и RRF для поиска и ранжирования документов:\n",
    "## После переформулировки вопрос передается в генерацию нескольких запросов, и система выполняет поиск документов для каждого из них."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Этот вариант работает медленнее и не сказал бы что ответы лучше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начните общение с ИИ! Введите ‘выход’, чтобы завершить разговор.\n",
      "Вы: кто ген дир?\n",
      "AI: Генеральным директором компании Intelion является Екатерина Евгеньевна Семыкина.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.load import dumps, loads\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Создаем модель ChatOpenAI с использованием модели GPT-4o\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")  # Модель GPT-4o для обработки запросов\n",
    "\n",
    "# Определяем системный промпт для контекстуализации вопросов\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Учитывая историю чата и последний вопрос пользователя, \"\n",
    "    \"который может ссылаться на контекст из истории чата, \"\n",
    "    \"сформулируйте самодостаточный вопрос, который можно понять \"\n",
    "    \"без использования истории чата. НЕ отвечайте на вопрос, просто \"\n",
    "    \"переформулируйте его, если это необходимо, или верните его без изменений.\"\n",
    ")\n",
    "\n",
    "# Создаем шаблон для формирования контекстуализированных вопросов\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Шаблон для генерации нескольких запросов\n",
    "template = \"\"\"Вы являетесь полезным помощником и знаете все о компании Intelion, который генерирует несколько поисковых запросов на основе одного входного запроса. \\n\n",
    "Сгенерируйте несколько поисковых запросов, связанных с: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Функция для генерации нескольких запросов\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | ChatOpenAI(temperature=0) \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "# Функция для Reciprocal Rank Fusion (RRF)\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\"Функция RRF для слияния результатов поиска из нескольких списков.\"\"\"\n",
    "    fused_scores = {}\n",
    "    for docs in results:\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "    \n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "    \n",
    "    return reranked_results\n",
    "\n",
    "# Создаем retrieval-цепочку с использованием ensemble_retriever\n",
    "def create_rag_fusion_retrieval_chain(ensemble_retriever, question):\n",
    "    # Генерируем несколько запросов\n",
    "    generated_queries = generate_queries.invoke({\"question\": question})\n",
    "    \n",
    "    # Выполняем поиск по базе данных для каждого запроса с использованием ensemble_retriever\n",
    "    # ensemble_retriever предполагает, что используется несколько различных поисковых движков\n",
    "    search_results = []\n",
    "    for query in generated_queries:\n",
    "        # Для каждого запроса вызываем ensemble_retriever и собираем результаты\n",
    "        retrieved_documents = ensemble_retriever.get_relevant_documents(query)\n",
    "        search_results.append(retrieved_documents)\n",
    "    \n",
    "    # Применяем RRF для слияния результатов\n",
    "    reranked_docs = reciprocal_rank_fusion(search_results)\n",
    "    \n",
    "    # Извлекаем только документы, без их рангов\n",
    "    return [doc for doc, score in reranked_docs]\n",
    "\n",
    "# Создаем шаблон для ответа на вопросы на основе контекста\n",
    "qa_system_prompt = (\n",
    "    \"Вы помощник для задач по ответам на вопросы и знаете все о компании Intelion. Используйте \"\n",
    "    \"следующие части полученного контекста, чтобы ответить на \"\n",
    "    \"вопрос. Если вы не знаете ответа, просто скажите, что вы \"\n",
    "    \"не знаете. Используйте максимум три предложения и дайте \"\n",
    "    \"краткий ответ.\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Функция для обработки запроса с контекстуализацией и RAG-Fusion\n",
    "def process_query(query, chat_history):\n",
    "    # Шаг 1: Переформулируем вопрос с учетом истории чата\n",
    "    reformulated_prompt = contextualize_q_prompt.format_messages(\n",
    "        chat_history=chat_history, input=query\n",
    "    )\n",
    "    reformulated_query_result = llm(reformulated_prompt)  # Получаем объект AIMessage\n",
    "    reformulated_query = reformulated_query_result.content  # Доступ к содержимому\n",
    "    \n",
    "    # Шаг 2: Получаем документы с использованием RAG-Fusion и RRF через ensemble_retriever\n",
    "    docs = create_rag_fusion_retrieval_chain(ensemble_retriever, reformulated_query)\n",
    "    \n",
    "    # Шаг 3: Генерируем ответ на основе контекста\n",
    "    result = question_answer_chain.invoke({\"input\": query, \"context\": docs, \"chat_history\": chat_history})\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Функция для моделирования непрерывного чата\n",
    "def continual_chat():\n",
    "    print(\"Начните общение с ИИ! Введите ‘выход’, чтобы завершить разговор.\")\n",
    "    chat_history = []  # Хранение истории чата\n",
    "    while True:\n",
    "        query = input(\"ВЫ: \")  # Получение запроса от пользователя\n",
    "        if query.lower() == \"выход\":  # Если пользователь хочет завершить чат\n",
    "            break\n",
    "        \n",
    "        # Обрабатываем запрос пользователя через систему RAG-Fusion с RRF\n",
    "        result = process_query(query, chat_history)\n",
    "        \n",
    "        # Печать вопроса пользователя и ответа ИИ\n",
    "        print(f\"Вы: {query}\")\n",
    "        print(f\"AI: {result}\")  # Выводим результат, так как это строка\n",
    "        \n",
    "        # Обновление истории чата\n",
    "        chat_history.append(HumanMessage(content=query))\n",
    "        chat_history.append(SystemMessage(content=result))\n",
    "\n",
    "# Запуск непрерывного чата\n",
    "if __name__ == \"__main__\":\n",
    "    continual_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оцениваем при помощи RAGAS работу наших RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем датасет из нашей базы данных с \n",
    " ### question — вопрос пользователя, который он подает в RAG;\n",
    " ### contexts — контексты, использовавшиеся при ответе на вопрос;\n",
    " ### ground_truth — верный ответ на вопрос пользователя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Шаг 1: Подготовка схемы и парсера для генерации вопросов\n",
    "question_schema = ResponseSchema(\n",
    "    name=\"question\",\n",
    "    description=\"вопрос о контексте.\"\n",
    ")\n",
    "\n",
    "question_response_schemas = [question_schema]\n",
    "question_output_parser = StructuredOutputParser.from_response_schemas(question_response_schemas)\n",
    "\n",
    "# GPT-3.5 модель с минимальными параметрами\n",
    "question_generation_llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\"  # Обновленный параметр 'model_name'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:29<00:00,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# Шаблон для генерации вопросов\n",
    "qa_template = \"\"\"\\\n",
    "Вы помощник для задач по ответам на вопросы о компании Intelion. Для каждого контекста создайте конкретный вопрос, который относится именно к предоставленному контексту. Избегайте общих или слишком обобщённых вопросов.\n",
    "\n",
    "question: вопрос, относящийся к контексту.\n",
    "\n",
    "Отформатируйте вывод в формате JSON с ключами:\n",
    "question\n",
    "\n",
    "context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template=qa_template)\n",
    "\n",
    "# Шаг 2: Генерация вопросов по первым 30 чанкам контекста\n",
    "qac_triples = []\n",
    "\n",
    "# Ограничим до первых 30 чанков\n",
    "for text in tqdm(docs[:30]):\n",
    "    # Форматирование сообщений для модели\n",
    "    messages = prompt_template.format_messages(\n",
    "        context=text.page_content\n",
    "    )\n",
    "    \n",
    "    # Вызов модели для генерации вопроса\n",
    "    response = question_generation_llm(messages)\n",
    "    try:\n",
    "        output_dict = question_output_parser.parse(response.content)\n",
    "    except Exception as e:\n",
    "        continue\n",
    "    # Добавляем в список: вопрос и контекст\n",
    "    output_dict[\"context\"] = text.page_content\n",
    "    qac_triples.append(output_dict)\n",
    "\n",
    "# Шаг 3: Подготовка схемы и парсера для генерации ответов\n",
    "answer_schema = ResponseSchema(\n",
    "    name=\"answer\",\n",
    "    description=\"ответ на вопрос\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:43<00:00,  1.45s/it]\n"
     ]
    }
   ],
   "source": [
    "answer_response_schemas = [answer_schema]\n",
    "answer_output_parser = StructuredOutputParser.from_response_schemas(answer_response_schemas)\n",
    "\n",
    "# Шаблон для генерации ответов\n",
    "qa_template = \"\"\"\\\n",
    "Вы помощник для задач по ответам на вопросы о компании Intelion. Для каждого вопроса и контекста создайте конкретный и точный ответ.\n",
    "\n",
    "answer: ответ на вопрос, относящийся к контексту.\n",
    "\n",
    "Отформатируйте вывод в формате JSON с ключами:\n",
    "answer\n",
    "\n",
    "question: {question}\n",
    "context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template=qa_template)\n",
    "\n",
    "# gpt-4o-mini модель для ответов\n",
    "answer_generation_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Шаг 4: Генерация ответов по каждому вопросу и контексту для первых 30 чанков\n",
    "for triple in tqdm(qac_triples):\n",
    "    messages = prompt_template.format_messages(\n",
    "        context=triple[\"context\"],\n",
    "        question=triple[\"question\"]\n",
    "    )\n",
    "    \n",
    "    # Вызов модели для генерации ответа\n",
    "    response = answer_generation_llm(messages)\n",
    "    try:\n",
    "        output_dict = answer_output_parser.parse(response.content)\n",
    "    except Exception as e:\n",
    "        continue\n",
    "    # Добавляем ответ в тройку данных\n",
    "    triple[\"answer\"] = output_dict[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Какая область деятельности компании Intelion?</td>\n",
       "      <td>﻿Общая информация о компании Intelion\\nДобрый ...</td>\n",
       "      <td>Компания Intelion занимается разработкой и вне...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Какая основная информация содержится в данном ...</td>\n",
       "      <td>В этом документе собрана основная информация, ...</td>\n",
       "      <td>В данном документе содержится основная информа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Какие основные сферы деятельности компании Int...</td>\n",
       "      <td>для первого знакомства с компанией. Другие воп...</td>\n",
       "      <td>Основные сферы деятельности компании Intelion ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Какую анкету следует использовать для добавлен...</td>\n",
       "      <td>Если вы хотите добавить какую-то информацию в ...</td>\n",
       "      <td>Воспользуйтесь предоставленной анкетой для доб...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Какие технологии в области искусственного инте...</td>\n",
       "      <td>данных, воспользуйтесь этой анкетой:  https://...</td>\n",
       "      <td>Компания Intelion использует различные техноло...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0      Какая область деятельности компании Intelion?   \n",
       "1  Какая основная информация содержится в данном ...   \n",
       "2  Какие основные сферы деятельности компании Int...   \n",
       "3  Какую анкету следует использовать для добавлен...   \n",
       "4  Какие технологии в области искусственного инте...   \n",
       "\n",
       "                                             context  \\\n",
       "0  ﻿Общая информация о компании Intelion\\nДобрый ...   \n",
       "1  В этом документе собрана основная информация, ...   \n",
       "2  для первого знакомства с компанией. Другие воп...   \n",
       "3  Если вы хотите добавить какую-то информацию в ...   \n",
       "4  данных, воспользуйтесь этой анкетой:  https://...   \n",
       "\n",
       "                                        ground_truth  \n",
       "0  Компания Intelion занимается разработкой и вне...  \n",
       "1  В данном документе содержится основная информа...  \n",
       "2  Основные сферы деятельности компании Intelion ...  \n",
       "3  Воспользуйтесь предоставленной анкетой для доб...  \n",
       "4  Компания Intelion использует различные техноло...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Шаг 5: Сохранение данных в pandas DataFrame\n",
    "ground_truth_qac_set = pd.DataFrame(qac_triples)\n",
    "\n",
    "# Сохранение контекста в строковом виде\n",
    "ground_truth_qac_set[\"context\"] = ground_truth_qac_set[\"context\"].map(lambda x: str(x))\n",
    "\n",
    "# Переименуем колонку с ответами для ясности\n",
    "ground_truth_qac_set = ground_truth_qac_set.rename(columns={\"answer\": \"ground_truth\"})\n",
    "\n",
    "# Выводим DataFrame\n",
    "ground_truth_qac_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Какая основная информация содержится в данном документе о компании Intelion?'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_qac_set['question'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'В этом документе собрана основная информация, необходимая для первого знакомства с компанией.'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_qac_set['context'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'В данном документе содержится основная информация о компании Intelion, которая может включать ее историю, миссию, основные направления деятельности, продукты и услуги, а также контактные данные для связи.'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_qac_set['ground_truth'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "eval_dataset = Dataset.from_pandas(ground_truth_qac_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'context', 'ground_truth'],\n",
       "    num_rows: 30\n",
       "})"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    answer_correctness,\n",
    "    answer_similarity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка простого RAG с использованием только поиска по векторной базе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:12,  1.24s/it]\n",
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 174.12ba/s]\n",
      "Evaluating: 100%|██████████| 60/60 [00:21<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context_precision': 0.0000, 'faithfulness': 0.3000, 'answer_relevancy': 0.7023, 'context_recall': 0.0000, 'answer_correctness': 0.4054, 'answer_similarity': 0.9098}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Оценка RAG-набора данных с использованием RAGAS\n",
    "def evaluate_ragas_dataset(ragas_dataset):\n",
    "    result = evaluate(\n",
    "        ragas_dataset,\n",
    "        metrics=[\n",
    "            context_precision,\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "            context_recall,\n",
    "            answer_correctness,\n",
    "            answer_similarity\n",
    "        ],\n",
    "    )\n",
    "    return result\n",
    "\n",
    "# Функция для создания RAG-набора данных с использованием цепочки simple_vector_store_chain\n",
    "def create_ragas_dataset_with_simple_chain(simple_chain, eval_dataset):\n",
    "    rag_dataset = []\n",
    "    \n",
    "    # Преобразуем Dataset в pandas DataFrame для удобной работы с данными\n",
    "    eval_df = pd.DataFrame(eval_dataset)\n",
    "    \n",
    "    for _, row in tqdm(eval_df.iterrows()):  # Итерируем через строки DataFrame\n",
    "        try:\n",
    "            # Вызов цепочки через метод invoke\n",
    "            result = simple_chain.invoke({\"input\": row[\"question\"], \"chat_history\": []})\n",
    "            \n",
    "            # Проверка на наличие ключей\n",
    "            if not result or \"answer\" not in result:\n",
    "                print(f\"Ошибка: отсутствует ключ 'answer' для вопроса: {row['question']}\")\n",
    "                continue\n",
    "            \n",
    "            # Получаем релевантные документы (если есть)\n",
    "            relevant_docs = result.get(\"relevant_docs\", [])\n",
    "            contexts = [doc.page_content for doc in relevant_docs]\n",
    "\n",
    "            # Добавление данных в RAG-набор\n",
    "            rag_dataset.append(\n",
    "                {\n",
    "                    \"question\": row[\"question\"],\n",
    "                    \"answer\": result[\"answer\"],  # Ответ от цепочки\n",
    "                    \"contexts\": contexts,  # Оставляем контексты в виде списка\n",
    "                    \"ground_truths\": row[\"ground_truth\"],  # Эталонные ответы\n",
    "                    \"reference\": row[\"ground_truth\"]  # Эталонный ответ для оценки\n",
    "                }\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка обработки вопроса '{row['question']}': {e}\")\n",
    "    \n",
    "    # Преобразуем итоговый набор в pandas DataFrame\n",
    "    if rag_dataset:  # Проверяем, что набор данных не пустой\n",
    "        rag_df = pd.DataFrame(rag_dataset)\n",
    "        rag_eval_dataset = Dataset.from_pandas(rag_df)\n",
    "        return rag_eval_dataset\n",
    "    else:\n",
    "        print(\"Ошибка: набор данных пустой.\")\n",
    "        return None\n",
    "\n",
    "# Создание RAG-набора данных для оценки с использованием цепочки simple_vector_store_chain (первые 10 строк)\n",
    "basic_qa_ragas_dataset_simple = create_ragas_dataset_with_simple_chain(simple_vector_store_chain, eval_dataset[:10])\n",
    "\n",
    "# Проверка, что создание RAG-набора данных завершилось успешно\n",
    "if basic_qa_ragas_dataset_simple is not None:\n",
    "    # Сохранение RAG-набора данных в CSV\n",
    "    basic_qa_ragas_dataset_simple.to_csv(\"basic_qa_ragas_dataset_simple.csv\")\n",
    "\n",
    "    # Оценка RAG-набора данных\n",
    "    basic_qa_vector_store = evaluate_ragas_dataset(basic_qa_ragas_dataset_simple)\n",
    "\n",
    "    # Вывод результата оценки\n",
    "    print(basic_qa_vector_store)\n",
    "else:\n",
    "    print(\"Ошибка создания RAG-набора данных. Оценка не будет выполнена.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка простого RAG с использованием поиска по векторной базе данных и ключевым словам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:11,  1.12s/it]\n",
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 298.06ba/s]\n",
      "Evaluating: 100%|██████████| 60/60 [00:21<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context_precision': 0.0000, 'faithfulness': 0.0000, 'answer_relevancy': 0.7049, 'context_recall': 0.0000, 'answer_correctness': 0.4183, 'answer_similarity': 0.9146}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Оценка RAG-набора данных с использованием RAGAS\n",
    "def evaluate_ragas_dataset(ragas_dataset):\n",
    "    result = evaluate(\n",
    "        ragas_dataset,\n",
    "        metrics=[\n",
    "            context_precision,\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "            context_recall,\n",
    "            answer_correctness,\n",
    "            answer_similarity\n",
    "        ],\n",
    "    )\n",
    "    return result\n",
    "\n",
    "# Функция для создания RAG-набора данных с использованием цепочки simple_vector_store_and_BM25\n",
    "def create_ragas_dataset_with_bm25_chain(bm25_chain, eval_dataset):\n",
    "    rag_dataset = []\n",
    "    \n",
    "    # Преобразуем Dataset в pandas DataFrame для удобной работы с данными\n",
    "    eval_df = pd.DataFrame(eval_dataset)\n",
    "    \n",
    "    for _, row in tqdm(eval_df.iterrows()):  # Итерируем через строки DataFrame\n",
    "        try:\n",
    "            # Вызов цепочки через метод invoke\n",
    "            result = bm25_chain.invoke({\"input\": row[\"question\"], \"chat_history\": []})\n",
    "            \n",
    "            # Проверка на наличие ключей\n",
    "            if not result or \"answer\" not in result:\n",
    "                print(f\"Ошибка: отсутствует ключ 'answer' для вопроса: {row['question']}\")\n",
    "                continue\n",
    "            \n",
    "            # Получаем релевантные документы (если есть)\n",
    "            relevant_docs = result.get(\"relevant_docs\", [])\n",
    "            contexts = [doc.page_content for doc in relevant_docs]\n",
    "\n",
    "            # Добавление данных в RAG-набор\n",
    "            rag_dataset.append(\n",
    "                {\n",
    "                    \"question\": row[\"question\"],\n",
    "                    \"answer\": result[\"answer\"],  # Ответ от цепочки\n",
    "                    \"contexts\": contexts,  # Оставляем контексты в виде списка\n",
    "                    \"ground_truths\": row[\"ground_truth\"],  # Эталонные ответы\n",
    "                    \"reference\": row[\"ground_truth\"]  # Эталонный ответ для оценки\n",
    "                }\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка обработки вопроса '{row['question']}': {e}\")\n",
    "    \n",
    "    # Преобразуем итоговый набор в pandas DataFrame\n",
    "    if rag_dataset:  # Проверяем, что набор данных не пустой\n",
    "        rag_df = pd.DataFrame(rag_dataset)\n",
    "        rag_eval_dataset = Dataset.from_pandas(rag_df)\n",
    "        return rag_eval_dataset\n",
    "    else:\n",
    "        print(\"Ошибка: набор данных пустой.\")\n",
    "        return None\n",
    "\n",
    "# Создание RAG-набора данных для оценки с использованием цепочки simple_vector_store_and_BM25 (первые 10 строк)\n",
    "basic_qa_ragas_dataset_bm25 = create_ragas_dataset_with_bm25_chain(simple_vector_store_and_BM25, eval_dataset[:10])\n",
    "\n",
    "# Проверка, что создание RAG-набора данных завершилось успешно\n",
    "if basic_qa_ragas_dataset_bm25 is not None:\n",
    "    # Сохранение RAG-набора данных в CSV\n",
    "    basic_qa_ragas_dataset_bm25.to_csv(\"basic_qa_ragas_dataset_bm25.csv\")\n",
    "\n",
    "    # Оценка RAG-набора данных\n",
    "    basic_qa_result_bm25 = evaluate_ragas_dataset(basic_qa_ragas_dataset_bm25)\n",
    "\n",
    "    # Вывод результата оценки\n",
    "    print(basic_qa_result_bm25)\n",
    "else:\n",
    "    print(\"Ошибка создания RAG-набора данных. Оценка не будет выполнена.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка RAG с использованием поиска по векторной базе данных, ключевым словам и переранжированием"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:14,  1.49s/it]\n",
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 162.34ba/s]\n",
      "Evaluating: 100%|██████████| 60/60 [01:46<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context_precision': 0.8408, 'faithfulness': 0.7750, 'answer_relevancy': 0.8986, 'context_recall': 0.7500, 'answer_correctness': 0.4878, 'answer_similarity': 0.9517}\n"
     ]
    }
   ],
   "source": [
    "# Оценка RAG-набора данных с использованием RAGAS\n",
    "def evaluate_ragas_dataset(ragas_dataset):\n",
    "    result = evaluate(\n",
    "        ragas_dataset,\n",
    "        metrics=[\n",
    "            context_precision,\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "            context_recall,\n",
    "            answer_correctness,\n",
    "            answer_similarity\n",
    "        ],\n",
    "    )\n",
    "    return result\n",
    "\n",
    "# Функция для создания RAG-набора данных с переранжированием\n",
    "def create_ragas_dataset_with_reranked(reranked_chain, eval_dataset):\n",
    "    rag_dataset = []\n",
    "    \n",
    "    # Преобразуем Dataset в pandas DataFrame\n",
    "    eval_df = pd.DataFrame(eval_dataset)\n",
    "    \n",
    "    for _, row in tqdm(eval_df.iterrows()):  # Итерируем через строки DataFrame\n",
    "        try:\n",
    "            # Выполнение переранжированной цепочки для каждого вопроса\n",
    "            result = reranked_chain(row[\"question\"], chat_history=[])\n",
    "            \n",
    "            # Проверка на наличие ключей\n",
    "            if not result or \"reordered_docs\" not in result or \"answer\" not in result:\n",
    "                print(f\"Ошибка: отсутствуют ключи 'reordered_docs' или 'answer' для вопроса: {row['question']}\")\n",
    "                continue\n",
    "            \n",
    "            # Проверка структуры документов\n",
    "            reordered_docs = result[\"reordered_docs\"]\n",
    "            contexts = [doc.page_content for doc in reordered_docs]\n",
    "\n",
    "            # Добавление данных в RAG-набор\n",
    "            rag_dataset.append(\n",
    "                {\n",
    "                    \"question\": row[\"question\"],\n",
    "                    \"answer\": result[\"answer\"],  # Ответ с использованием переранжирования\n",
    "                    \"contexts\": contexts,  # Оставляем контексты в виде списка\n",
    "                    \"ground_truths\": row[\"ground_truth\"],  # Эталонные ответы\n",
    "                    \"reference\": row[\"ground_truth\"]  # Эталонный ответ для оценки\n",
    "                }\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка обработки вопроса '{row['question']}': {e}\")\n",
    "    \n",
    "    # Преобразуем итоговый набор в Dataset\n",
    "    rag_df = pd.DataFrame(rag_dataset)\n",
    "    \n",
    "    rag_eval_dataset = Dataset.from_pandas(rag_df)\n",
    "    \n",
    "    return rag_eval_dataset\n",
    "\n",
    "# Создание RAG-набора данных для оценки с использованием переранжированной цепочки (только первые 10 строк)\n",
    "basic_qa_ragas_dataset_reranked = create_ragas_dataset_with_reranked(simple_reranked_chain, eval_dataset[:10])\n",
    "\n",
    "# Проверка, что создание RAG-набора данных завершилось успешно\n",
    "if basic_qa_ragas_dataset_reranked is not None:\n",
    "    # Сохранение RAG-набора данных в CSV\n",
    "    basic_qa_ragas_dataset_reranked.to_csv(\"basic_qa_ragas_dataset_reranked.csv\")\n",
    "\n",
    "    # Оценка RAG-набора данных\n",
    "    basic_qa_result_reranked = evaluate_ragas_dataset(basic_qa_ragas_dataset_reranked)\n",
    "\n",
    "    # Вывод результата оценки\n",
    "    print(basic_qa_result_reranked)\n",
    "else:\n",
    "    print(\"Ошибка создания RAG-набора данных. Оценка не будет выполнена.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка RAG с использованием поиска по векторной базе данных, ключевым словам и RAG-Fusion и RRF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:40,  4.02s/it]\n",
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 340.03ba/s]\n",
      "Evaluating: 100%|██████████| 60/60 [00:19<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context_precision': 0.0000, 'faithfulness': 0.3500, 'answer_relevancy': 0.7975, 'context_recall': 0.0000, 'answer_correctness': 0.4891, 'answer_similarity': 0.9485}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Оценка RAG-набора данных с использованием RAGAS\n",
    "def evaluate_ragas_dataset(ragas_dataset):\n",
    "    result = evaluate(\n",
    "        ragas_dataset,\n",
    "        metrics=[\n",
    "            context_precision,\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "            context_recall,\n",
    "            answer_correctness,\n",
    "            answer_similarity\n",
    "        ],\n",
    "    )\n",
    "    return result\n",
    "\n",
    "# Функция для создания RAG-набора данных с использованием цепочки RAG-Fusion\n",
    "def create_ragas_dataset_with_fusion_chain(fusion_chain, eval_dataset):\n",
    "    rag_dataset = []\n",
    "    \n",
    "    # Преобразуем Dataset в pandas DataFrame для удобной работы с данными\n",
    "    eval_df = pd.DataFrame(eval_dataset)\n",
    "    \n",
    "    for _, row in tqdm(eval_df.iterrows()):  # Итерируем через строки DataFrame\n",
    "        try:\n",
    "            # Вызов функции напрямую, передавая вопрос и пустую историю чата\n",
    "            result = fusion_chain(row[\"question\"], [])\n",
    "            \n",
    "            # Получаем релевантные документы (если есть)\n",
    "            # Если результат — это строка, используем его напрямую как ответ\n",
    "            answer = result if isinstance(result, str) else result.get(\"answer\", \"\")\n",
    "            relevant_docs = result.get(\"relevant_docs\", []) if not isinstance(result, str) else []\n",
    "            contexts = [doc.page_content for doc in relevant_docs]\n",
    "\n",
    "            # Добавление данных в RAG-набор\n",
    "            rag_dataset.append(\n",
    "                {\n",
    "                    \"question\": row[\"question\"],\n",
    "                    \"answer\": answer,  # Используем строковый ответ\n",
    "                    \"contexts\": contexts,  # Оставляем контексты в виде списка\n",
    "                    \"ground_truths\": row[\"ground_truth\"],  # Эталонные ответы\n",
    "                    \"reference\": row[\"ground_truth\"]  # Эталонный ответ для оценки\n",
    "                }\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка обработки вопроса '{row['question']}': {e}\")\n",
    "    \n",
    "    # Преобразуем итоговый набор в pandas DataFrame\n",
    "    if rag_dataset:  # Проверяем, что набор данных не пустой\n",
    "        rag_df = pd.DataFrame(rag_dataset)\n",
    "        rag_eval_dataset = Dataset.from_pandas(rag_df)\n",
    "        return rag_eval_dataset\n",
    "    else:\n",
    "        print(\"Ошибка: набор данных пустой.\")\n",
    "        return None\n",
    "\n",
    "# Создание RAG-набора данных для оценки с использованием цепочки RAG-Fusion (первые 10 строк)\n",
    "basic_qa_ragas_dataset_fusion = create_ragas_dataset_with_fusion_chain(process_query, eval_dataset[:10])\n",
    "\n",
    "# Проверка, что создание RAG-набора данных завершилось успешно\n",
    "if basic_qa_ragas_dataset_fusion is not None:\n",
    "    # Сохранение RAG-набора данных в CSV\n",
    "    basic_qa_ragas_dataset_fusion.to_csv(\"basic_qa_ragas_dataset_fusion.csv\")\n",
    "\n",
    "    # Оценка RAG-набора данных\n",
    "    basic_qa_result_fusion_RRF= evaluate_ragas_dataset(basic_qa_ragas_dataset_fusion)\n",
    "\n",
    "    # Вывод результата оценки\n",
    "    print(basic_qa_result_fusion_RRF)\n",
    "else:\n",
    "    print(\"Ошибка создания RAG-набора данных. Оценка не будет выполнена.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\t•\tRRF — это мощный метод, который использует ранжирование результатов из разных источников для слияния в единый список. Однако, если результирующие документы, полученные через ensemble_retriever, имеют высокую степень схожести или одинаковые данные (из-за пересечения в запросах), RRF может вносить дополнительный “шум” в ранжирование и сделать его менее четким.\n",
    "\t•\tГенерация нескольких запросов и их последующий анализ не всегда гарантирует улучшение, особенно если поисковые движки, используемые ensemble_retriever, имеют сходные индексы. Это может приводить к дублированию информации или незначительным различиям, что не увеличивает разнообразие результатов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>answer_correctness</th>\n",
       "      <th>answer_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>basic_qa_vector_store</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.702250</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.405394</td>\n",
       "      <td>0.909810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>basic_qa_result_bm25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.704887</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.418296</td>\n",
       "      <td>0.914565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>basic_qa_result_reranked</td>\n",
       "      <td>0.840822</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.898564</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.487764</td>\n",
       "      <td>0.951729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basic_qa_result_fusion_RRF</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.797542</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.489093</td>\n",
       "      <td>0.948525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         name  context_precision  faithfulness  \\\n",
       "0       basic_qa_vector_store           0.000000         0.300   \n",
       "1        basic_qa_result_bm25           0.000000         0.000   \n",
       "2    basic_qa_result_reranked           0.840822         0.775   \n",
       "3  basic_qa_result_fusion_RRF           0.000000         0.350   \n",
       "\n",
       "   answer_relevancy  context_recall  answer_correctness  answer_similarity  \n",
       "0          0.702250            0.00            0.405394           0.909810  \n",
       "1          0.704887            0.00            0.418296           0.914565  \n",
       "2          0.898564            0.75            0.487764           0.951729  \n",
       "3          0.797542            0.00            0.489093           0.948525  "
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создание списка данных и имен\n",
    "data = [\n",
    "    ('basic_qa_vector_store', basic_qa_vector_store),\n",
    "    ('basic_qa_result_bm25', basic_qa_result_bm25),\n",
    "    ('basic_qa_result_reranked', basic_qa_result_reranked),\n",
    "    ('basic_qa_result_fusion_RRF', basic_qa_result_fusion_RRF),\n",
    "]\n",
    "\n",
    "# Создание DataFrame из каждого словаря с добавлением имени\n",
    "rows = []\n",
    "for name, values in data:\n",
    "    row = {'name': name}\n",
    "    row.update(values)\n",
    "    rows.append(row)\n",
    "\n",
    "# Создание финального DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Печать таблицы\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sergey/Desktop/RAG_Project_FULL/RAG_intelion/Intelion_books/retrieval_chain_results.csv'"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Данные таблицы\n",
    "data = {\n",
    "    \"name\": [\"basic_qa_vector_store\", \"basic_qa_result_bm25\", \"basic_qa_result_reranked\", \"basic_qa_result_fusion_RRF\"],\n",
    "    \"context_precision\": [0.0, 0.0, 0.840822, 0.0],\n",
    "    \"faithfulness\": [0.3, 0.0, 0.775, 0.35],\n",
    "    \"answer_relevancy\": [0.70225, 0.704887, 0.898564, 0.797542],\n",
    "    \"context_recall\": [0.0, 0.0, 0.75, 0.0],\n",
    "    \"answer_correctness\": [0.405394, 0.418296, 0.487764, 0.489093],\n",
    "    \"answer_similarity\": [0.90981, 0.914565, 0.951729, 0.948525]\n",
    "}\n",
    "\n",
    "# Создание DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Сохранение таблицы в CSV файл\n",
    "csv_path = \"/Users/sergey/Desktop/RAG_Project_FULL/RAG_intelion/Intelion_books/retrieval_chain_results.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "csv_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
