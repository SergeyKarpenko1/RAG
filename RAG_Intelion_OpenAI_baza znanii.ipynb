{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка максимального количества выводимых строк в DataFrame\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка и чтение файлов в формате txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Получаем текущий рабочий каталог\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Определяем путь к файлу\n",
    "file_path = os.path.join(current_dir, \"/Users/sergey/Desktop/RAG_Project/RAG_intelion/Intelion_books/bazaznanii.txt\")\n",
    "\n",
    "# Определяем путь к директории для хранилища\n",
    "persistent_directory = os.path.join(current_dir, \"db\", \"chroma_db_bazaznanii\")\n",
    "\n",
    "\n",
    "db_dir = os.path.join(current_dir, \"db\")\n",
    "\n",
    "# Проверка, существует ли текстовый файл, с которого нужно загружать данные\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"Файл {file_path} не найден. Проверьте путь.\")\n",
    "else:\n",
    "    # Если файл найден, загружаем его содержимое с помощью загрузчика текстовых данных\n",
    "    loader = TextLoader(file_path)\n",
    "    documents = loader.load()  # Загружаем документы из текстового файла\n",
    "    print(f\"Загружено {len(documents)} документов.\")\n",
    "\n",
    "# Если документы загружены, используем рекурсивный сплиттер для разбиения\n",
    "print(\"\\n--- Используем рекурсивное разбиение на чанки ---\")\n",
    "Recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100, chunk_overlap=30\n",
    ")\n",
    "\n",
    "# Разбиваем документы на чанки\n",
    "docs = Recursive_text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Разбито на {len(docs)} чанков.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объединение пути к текущему каталогу с подкаталогом \"db\" и именем каталога \"chroma_db\"\n",
    "# Это создает полный путь к каталогу для хранения данных \"chroma_db\"\n",
    "# persistent_directory = os.path.join(current_dir, \"db\", \"chroma_db_intelion_PDF\")\n",
    "\n",
    "# db_dir = os.path.join(current_dir, \"db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# В качестве эмбеддингов используем text-embedding-3-small от OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем модель эмбеддингов\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для создания и сохранения векторного хранилища\n",
    "def create_vector_store(docs, store_name):\n",
    "    \n",
    "    # Определение пути к директории, где будет сохранено векторное хранилище\n",
    "    persistent_directory = os.path.join(db_dir, store_name)\n",
    "    \n",
    "    # Проверка, существует ли уже директория для хранения векторного хранилища\n",
    "    if not os.path.exists(persistent_directory):\n",
    "        \n",
    "        # Если директория не существует, выводим сообщение о создании векторного хранилища\n",
    "        print(f\"\\n--- Создание векторного хранилища {store_name} ---\")\n",
    "        \n",
    "        # Создаем векторное хранилище из документов с использованием заданной модели embeddings\n",
    "        # и сохраняем его в указанную директорию\n",
    "        db = Chroma.from_documents(\n",
    "            docs, embeddings, persist_directory=persistent_directory\n",
    "        )\n",
    "        \n",
    "        # Сообщаем, что создание векторного хранилища завершено\n",
    "        print(f\"--- Создание векторного хранилища завершено {store_name} ---\")\n",
    "    \n",
    "    # Если директория уже существует, выводим сообщение, что хранилище уже создано, и инициализация не требуется\n",
    "    else:\n",
    "        print(\n",
    "            f\"Хранилище {store_name} хранилище уже создано, и инициализация не требуется.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создаем векторную базу данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "create_vector_store(docs, \"chroma_db_intelion_bazaznanii\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка векторной базы данных после ее создания или если она была создана ранее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# from chromadb import Chroma  # Импортируем библиотеку Chroma\n",
    "\n",
    "# Укажите путь к вашей базе данных\n",
    "persistent_directory = \"/Users/sergey/Desktop/RAG_Project/RAG_intelion/db/chroma_db_intelion_bazaznanii\"\n",
    "\n",
    "# Проверьте, существует ли база данных\n",
    "if os.path.exists(os.path.join(persistent_directory, \"chroma.sqlite3\")):\n",
    "    print(f\"Загружаем векторное хранилище из {persistent_directory}...\")\n",
    "\n",
    "    # Загружаем векторное хранилище\n",
    "    db = Chroma(\n",
    "        persist_directory=persistent_directory,  # Указываем путь к существующему хранилищу\n",
    "        embedding_function=embeddings  # Передаем функцию эмбеддингов (нужна для поиска)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проверяем как работает поиск по базе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Результаты поиска по сходству (без порога) ---\n",
      "Документ 1:\n",
      "ЕС – Екатерина Евгеньевна Семыкина, Заместитель генерального директора\n",
      "\n",
      "Документ 2:\n",
      "ТА – Тимофей Андреевич Семенов, Генеральный директор\n",
      "\n",
      "Документ 3:\n",
      "поздразделения, то обращаться нужно напрямую к генеральному директору или к HRD Анастасии Нурмамед.\n",
      "\n",
      "\n",
      "--- Результаты поиска с порогом сходства ---\n",
      "Документ 3:\n",
      "ЕС – Екатерина Евгеньевна Семыкина, Заместитель генерального директора\n",
      "\n",
      "Документ 4:\n",
      "ТА – Тимофей Андреевич Семенов, Генеральный директор\n",
      "\n",
      "Документ 5:\n",
      "поздразделения, то обращаться нужно напрямую к генеральному директору или к HRD Анастасии Нурмамед.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Определяем вопрос, на который будем искать ответ\n",
    "query = \"Кто является генеральным директором?\"\n",
    "\n",
    "# 1. Поиск по сходству без порога (search_type=\"similarity\")\n",
    "retriever_similarity = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3},\n",
    ")\n",
    "\n",
    "# Выполняем поиск\n",
    "relevant_docs_similarity = retriever_similarity.invoke(query)\n",
    "\n",
    "# Вывод результатов\n",
    "print(\"\\n--- Результаты поиска по сходству (без порога) ---\")\n",
    "for i, doc in enumerate(relevant_docs_similarity, 1):\n",
    "    print(f\"Документ {i}:\\n{doc.page_content}\\n\")\n",
    "\n",
    "# 2. Поиск с использованием функции search с порогом сходства\n",
    "def search(query: str, db, k=3, similarity_threshold=0.4):\n",
    "    # Поиск документов с оценками релевантности\n",
    "    docs_scores = db.similarity_search_with_relevance_scores(query, k=k)\n",
    "    # Фильтрация по порогу сходства\n",
    "    good_documents = []\n",
    "    for doc, score in docs_scores:\n",
    "        if score > similarity_threshold:\n",
    "            good_documents.append(doc.page_content)\n",
    "    return good_documents\n",
    "\n",
    "# Выполняем поиск с порогом\n",
    "results_with_threshold = search(query, db=db, k=3, similarity_threshold=0.2)\n",
    "\n",
    "# Вывод результатов\n",
    "print(\"\\n--- Результаты поиска с порогом сходства ---\")\n",
    "for i, content in enumerate(results_with_threshold, 3):\n",
    "    print(f\"Документ {i}:\\n{content}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Здесь добовляем генерацию ответа LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Релевантные документы ---\n",
      "Документы 1:\n",
      "ЕС – Екатерина Евгеньевна Семыкина, Заместитель генерального директора\n",
      "\n",
      "Документы 2:\n",
      "ТА – Тимофей Андреевич Семенов, Генеральный директор\n",
      "\n",
      "Документы 3:\n",
      "поздразделения, то обращаться нужно напрямую к генеральному директору или к HRD Анастасии Нурмамед.\n",
      "\n",
      "Документы 4:\n",
      "АР – Александр Александрович Рощин, Операционный директор\n",
      "\n",
      "Документы 5:\n",
      "Операционный директор Александр Рощин\n",
      "\n",
      "\n",
      "--- Сгенерированный ответ ---\n",
      "Текстовый ответ модели:\n",
      "Генеральным директором является Тимофей Андреевич Семенов.\n"
     ]
    }
   ],
   "source": [
    "# Определяем вопрос, на который будем искать ответ\n",
    "query = \"Кто является генеральным директором?\"\n",
    "\n",
    "# Получаем релевантные документы на основе запроса\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",  # Указываем тип поиска по сходству\n",
    "    search_kwargs={\"k\": 5},  # Параметр: количество возвращаемых документов (k)\n",
    ")\n",
    "\n",
    "# Выполняем запрос к ретриверу для получения релевантных документов\n",
    "relevant_docs = retriever.invoke(query)\n",
    "\n",
    "# Выводим релевантные документы с их содержимым\n",
    "print(\"\\n--- Релевантные документы ---\")\n",
    "for i, doc in enumerate(relevant_docs, 1):\n",
    "    # Выводим каждый документ с его содержимым и номером\n",
    "    print(f\"Документы {i}:\\n{doc.page_content}\\n\")\n",
    "\n",
    "# Объединяем запрос и содержимое релевантных документов в один текст для передачи модели\n",
    "combined_input = (\n",
    "    \"Это некоторые документы которые могут помочь ответить на вопрос: \"\n",
    "    + query  # Включаем исходный запрос\n",
    "    + \"\\n\\nРелевантные документы:\\n\"  # Добавляем заголовок для раздела с документами\n",
    "    + \"\\n\\n\".join([doc.page_content for doc in relevant_docs])  # Добавляем текст всех документов\n",
    "    + \"\\n\\nПожалуйста, предоставьте ответ, основываясь только на предоставленных документах. Если ответ не найден в документах, ответьте ‘Я не уверен'.\"  # Просим модель ответить только на основе предоставленных данных\n",
    ")\n",
    "\n",
    "\n",
    "# Создаем модель ChatOpenAI для обработки запроса\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Определяем сообщения, которые передаем модели\n",
    "messages = [\n",
    "    SystemMessage(content=\"Ты помошник который отвечает на вопросы пользователя.\"),  # Системное сообщение с инструкциями для модели\n",
    "    HumanMessage(content=combined_input),  # Сообщение пользователя, содержащее запрос и документы\n",
    "]\n",
    "\n",
    "# Вызываем модель с подготовленными сообщениями\n",
    "result = model.invoke(messages)\n",
    "\n",
    "# Выводим сгенерированный ответ\n",
    "print(\"\\n--- Сгенерированный ответ ---\")\n",
    "# print(\"Full result:\")  # Можно также вывести полный результат, если нужно\n",
    "# print(result)\n",
    "print(\"Текстовый ответ модели:\")  # Выводим только текстовый ответ модели\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вариант с выводом перефразированных вопросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем retriever с типом поиска по схожести, чтобы получать k=3 наиболее релевантных результатов\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",  # Используем метод поиска по схожести\n",
    "    search_kwargs={\"k\": 5},  # Количество возвращаемых результатов: 3\n",
    ")\n",
    "\n",
    "# Создаем модель ChatOpenAI с использованием модели GPT-4o\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")  # Модель GPT-4o для обработки запросов\n",
    "\n",
    "# Определяем системный промпт для контекстуализации вопросов\n",
    "# Этот промпт объясняет ИИ, как следует формулировать вопрос на основе истории чата\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Учитывая историю чата и последний вопрос пользователя, \"  # Описание задачи\n",
    "    \"который может ссылаться на контекст из истории чата, \"  # Учитывание истории чата\n",
    "    \"сформулируйте самодостаточный вопрос, который можно понять \"  # Требование сделать вопрос самодостаточным\n",
    "    \"без использования истории чата. НЕ отвечайте на вопрос, просто \"  # Не нужно отвечать, только переформулировать вопрос\n",
    "    \"переформулируйте его, если это необходимо, или верните его без изменений.\"\n",
    "# )\n",
    "    # \"Вы являетесь ассистентом по языковой модели искусственного интеллекта.\"\n",
    "    # \"Ваша задача - сгенерировать 3 различных подвопросов Или альтернативных версий данного пользовательского вопроса для извлечения соответствующих документов из векторной базы данных.\"\n",
    "    # \"Сгенерировав несколько версий пользовательского вопроса, ваша цель - помочь пользователю преодолеть некоторые ограничения поиска сходства на основе расстояния.\"\n",
    "    # \"Создавая дополнительные вопросы, вы можете разбить вопросы, относящиеся к нескольким концепциям, на отдельные вопросы.\"\n",
    "    # \"Это поможет вам получить соответствующие документы для составления окончательного ответа\"\n",
    "    # \"Если в вопросе присутствует несколько концепций, вам следует разбить его на подзадачи, задав по одному вопросу для каждой концепции\"\n",
    "    )\n",
    "\n",
    "# Создаем шаблон для формирования контекстуализированных вопросов\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),  # Системное сообщение с инструкцией для ИИ\n",
    "        MessagesPlaceholder(\"chat_history\"),  # Место для истории чата\n",
    "        (\"human\", \"{input}\"),  # Ввод пользователя, который нужно переформулировать\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Создаем retriever, учитывающий историю чата\n",
    "# Этот retriever использует LLM для помощи в переформулировании вопросов с учетом контекста\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt  # Используем созданный шаблон и retriever для поиска с историей\n",
    ")\n",
    "\n",
    "# Определяем системный промпт для ответа на вопросы\n",
    "# Этот промпт объясняет ИИ, как отвечать на вопросы на основе контекста и что делать, если ответа нет\n",
    "qa_system_prompt = (\n",
    "    \"Вы помощник для задач по ответам на вопросы о людях из русской Wikipedia. Используйте \"  # Инструкция для ИИ\n",
    "    \"следующие части полученного контекста, чтобы ответить на \"  # Использование контекста для ответа\n",
    "    \"вопрос. Если вы не знаете ответа, просто скажите, что вы \"  # Если ответа нет, сообщить об этом\n",
    "    \"не знаете. Используйте максимум три предложения и дайте \"  # Ответ должен быть кратким\n",
    "    \"краткий ответ.\"  # Старайтесь отвечать как можно более сжато\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"  # Контекст, который будет использован для ответа\n",
    ")\n",
    "\n",
    "# Создаем шаблон для формирования ответов на вопросы\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),  # Системное сообщение с инструкцией для ИИ\n",
    "        MessagesPlaceholder(\"chat_history\"),  # Место для истории чата\n",
    "        (\"human\", \"{input}\"),  # Ввод пользователя, на который нужно ответить\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Создаем цепочку для объединения документов для ответов на вопросы\n",
    "# `create_stuff_documents_chain` передает весь найденный контекст в LLM для обработки\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "# Создаем retrieval-цепочку, которая объединяет retriever с историей и цепочку для ответов на вопросы\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "\n",
    "# # Функция для моделирования непрерывного чата\n",
    "# def continual_chat():\n",
    "#     print(\"Начните общение с ИИ! Введите ‘выход’, чтобы завершить разговор.\")  # Начало общения с ИИ\n",
    "#     chat_history = []  # Хранение истории чата\n",
    "#     while True:\n",
    "#         query = input(\"ВЫ: \")  # Получение запроса от пользователя\n",
    "#         if query.lower() == \"выход\":  # Если пользователь хочет завершить чат\n",
    "#             break\n",
    "#         # Обработка запроса пользователя через цепочку retrieval\n",
    "#         result = rag_chain.invoke({\"input\": query, \"chat_history\": chat_history})  # Запрос к цепочке\n",
    "#         # Печать вопроса пользователя и ответа ИИ\n",
    "#         print(f\"Вы: {query}\")  # Печать вопроса пользователя\n",
    "#         print(f\"AI: {result['answer']}\")  # Ответ ИИ\n",
    "#         # Обновление истории чата\n",
    "#         chat_history.append(HumanMessage(content=query))  # Добавление сообщения пользователя в историю\n",
    "#         chat_history.append(SystemMessage(content=result[\"answer\"]))  # Добавление ответа ИИ в историю\n",
    "\n",
    "# # Main function to start the continual chat\n",
    "# if __name__ == \"__main__\":\n",
    "#     continual_chat()\n",
    "\n",
    "\n",
    "# Функция для моделирования непрерывного чата с выводом переформулированного вопроса\n",
    "def continual_chat():\n",
    "    print(\"Начните общение с ИИ помошником о компании Intellion! Введите ‘выход’, чтобы завершить разговор.\")  # Начало общения с ИИ\n",
    "    chat_history = []  # Хранение истории чата\n",
    "    while True:\n",
    "        query = input(\"ВЫ: \")  # Получение запроса от пользователя\n",
    "        if query.lower() == \"выход\":  # Если пользователь хочет завершить чат\n",
    "            break\n",
    "\n",
    "        # Переформулировка вопроса через вызов LLM с контекстом\n",
    "        reformulated_prompt = contextualize_q_prompt.format_messages(\n",
    "            chat_history=chat_history, input=query\n",
    "        )\n",
    "        reformulated_query_result = llm(reformulated_prompt)  # Получаем объект AIMessage\n",
    "        reformulated_query = reformulated_query_result.content  # Доступ к содержимому через атрибут content\n",
    "        \n",
    "        print(f\"Переформулированный вопрос: {reformulated_query}\")  # Вывод переформулированного вопроса\n",
    "        \n",
    "        # Обработка запроса пользователя через цепочку retrieval\n",
    "        result = rag_chain.invoke({\"input\": reformulated_query, \"chat_history\": chat_history})  # Запрос к цепочке\n",
    "        \n",
    "        # Печать вопроса пользователя и ответа ИИ\n",
    "        print(f\"Вы: {query}\")  # Печать вопроса пользователя\n",
    "        print(f\"AI: {result['answer']}\")  # Ответ ИИ\n",
    "        \n",
    "        # Обновление истории чата\n",
    "        chat_history.append(HumanMessage(content=query))  # Добавление сообщения пользователя в историю\n",
    "        chat_history.append(SystemMessage(content=result[\"answer\"]))  # Добавление ответа ИИ в историю\n",
    "\n",
    "# Main function to start the continual chat\n",
    "if __name__ == \"__main__\":\n",
    "    continual_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import Tool\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_structured_chat_agent, create_react_agent\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain_core.messages import AIMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respond to the human as helpfully and accurately as possible. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\n",
    "\n",
    "Valid \"action\" values: \"Final Answer\" or {tool_names}\n",
    "\n",
    "Provide only ONE action per $JSON_BLOB, as shown:\n",
    "\n",
    "```\n",
    "{{\n",
    "  \"action\": $TOOL_NAME,\n",
    "  \"action_input\": $INPUT\n",
    "}}\n",
    "```\n",
    "\n",
    "Follow this format:\n",
    "\n",
    "Question: input question to answer\n",
    "Thought: consider previous and subsequent steps\n",
    "Action:\n",
    "```\n",
    "$JSON_BLOB\n",
    "```\n",
    "Observation: action result\n",
    "... (repeat Thought/Action/Observation N times)\n",
    "Thought: I know what to respond\n",
    "Action:\n",
    "```\n",
    "{{\n",
    "  \"action\": \"Final Answer\",\n",
    "  \"action_input\": \"Final response to human\"\n",
    "}}\n",
    "\n",
    "Begin! Reminder to ALWAYS respond with a valid json blob of a single action. Use tools if necessary. Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Здесь я использовал  ReAct Docstore для взаимодействия с пользователем (как он выглядит видно выше)\n",
    "\n",
    "## Результат хороший но используется доп API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем существующее хранилище векторов с функцией эмбеддинга\n",
    "db = Chroma(persist_directory=persistent_directory,\n",
    "            embedding_function=embeddings)  # Инициализируем хранилище векторов (Chroma) с указанием директории для хранения и функции эмбеддинга\n",
    "\n",
    "# Создаем retriever для поиска по хранилищу векторов\n",
    "# `search_type` указывает тип поиска (например, по схожести)\n",
    "# `search_kwargs` содержит дополнительные параметры поиска (например, количество возвращаемых результатов)\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",  # Указываем тип поиска — по схожести\n",
    "    search_kwargs={\"k\": 3},  # Возвращаем 3 наиболее релевантных результата\n",
    ")\n",
    "\n",
    "# Создаем модель ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", openai_api_key=os.getenv(\"OPENAI_API_KEY\"))   # Используем модель GPT-4o для работы с текстовыми данными\n",
    "\n",
    "# Определяем системный промпт для контекстуализации вопросов\n",
    "# Этот промпт помогает ИИ понять, что необходимо переформулировать вопрос\n",
    "# на основе истории чата, чтобы сделать его самодостаточным\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Учитывая историю чата и последний вопрос пользователя, \"\n",
    "    \"который может ссылаться на контекст из истории чата, \"\n",
    "    \"сформулируйте самодостаточный вопрос, который можно понять \"\n",
    "    \"без использования истории чата. НЕ отвечайте на вопрос, просто \"\n",
    "    \"переформулируйте его, если это необходимо, или верните его без изменений.\"\n",
    ")\n",
    "\n",
    "# Создаем шаблон для контекстуализации вопросов\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),  # Системное сообщение с инструкцией\n",
    "        MessagesPlaceholder(\"chat_history\"),  # Место для подстановки истории чата\n",
    "        (\"human\", \"{input}\"),  # Ввод пользователя, который нужно переформулировать\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Создаем retriever, учитывающий историю чата\n",
    "# Этот retriever использует LLM для переформулирования вопросов на основе истории чата\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt  # Используем созданный шаблон и retriever для поиска с историей\n",
    ")\n",
    "\n",
    "# Определяем системный промпт для ответа на вопросы\n",
    "# Этот промпт помогает ИИ давать краткие ответы, основываясь на полученном контексте\n",
    "# и указывает, что делать, если ответа нет\n",
    "qa_system_prompt = (\n",
    "    \"Вы помощник для задач по ответам на вопросы о компании Intelion. Используйте \"\n",
    "    \"следующие части полученного контекста, чтобы ответить на \"\n",
    "    \"вопрос. Если вы не знаете ответа, просто скажите, что вы \"\n",
    "    \"не знаете. Используйте максимум три предложения и дайте \"\n",
    "    \"краткий ответ.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"  # Подстановка контекста для ответа\n",
    ")\n",
    "\n",
    "# Создаем шаблон для формирования ответов на вопросы\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),  # Системное сообщение с инструкциями для ответа\n",
    "        MessagesPlaceholder(\"chat_history\"),  # История чата\n",
    "        (\"human\", \"{input}\"),  # Ввод пользователя\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Создаем цепочку для объединения документов для ответов на вопросы\n",
    "# `create_stuff_documents_chain` передает весь полученный контекст в LLM для обработки\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "# Создаем retrieval-цепочку, которая объединяет retriever с историей и цепочку для ответов на вопросы\n",
    "rag_chain = create_retrieval_chain(\n",
    "    history_aware_retriever, question_answer_chain  # Комбинируем поиск и цепочку ответа на вопросы\n",
    ")\n",
    "\n",
    "# Настройка ReAct-агента с использованием хранилища документов для поиска\n",
    "# Загружаем промпт ReAct Docstore для взаимодействия с пользователем через документы\n",
    "react_docstore_prompt = hub.pull(\"hwchase17/react\", api_key=os.getenv(\"LANGCHAIN_API_KEY\"))  # Промпт для ReAct-агента\n",
    "\n",
    "# Определяем инструменты, которые агент может использовать\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Answer Question\",  # Название инструмента\n",
    "        func=lambda input, **kwargs: rag_chain.invoke(\n",
    "            {\"input\": input, \"chat_history\": kwargs.get(\"chat_history\", [])}\n",
    "        ),  # Вызов функции для получения ответа с использованием цепочки поиска и истории\n",
    "        description=\"Полезно для ответов на вопросы по контексту.\",  # Описание инструмента\n",
    "    )\n",
    "]\n",
    "\n",
    "# Создаем ReAct-агента с использованием хранилища документов\n",
    "agent = create_react_agent(\n",
    "    llm=llm,  # Языковая модель\n",
    "    tools=tools,  # Инструменты, доступные агенту\n",
    "    prompt=react_docstore_prompt,  # Промпт для взаимодействия\n",
    ")\n",
    "\n",
    "# Инициализируем AgentExecutor для управления взаимодействием между пользователем, агентом и инструментами\n",
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent, tools=tools, handle_parsing_errors=True, verbose=True,  # Обработка ошибок и вывод процесса\n",
    ")\n",
    "\n",
    "# Инициализируем историю чата\n",
    "chat_history = []\n",
    "while True:\n",
    "    query = input(\"Вы: \")  # Получаем ввод пользователя\n",
    "    if query.lower() == \"выход\":  # Если введен 'exit', завершаем цикл\n",
    "        break\n",
    "    response = agent_executor.invoke(\n",
    "        {\"input\": query, \"chat_history\": chat_history})  # Агент обрабатывает запрос с учетом истории чата\n",
    "    print(f\"ИИ: {response['output']}\")  # Выводим ответ агента\n",
    "\n",
    "    # Обновляем историю чата\n",
    "    chat_history.append(HumanMessage(content=query))  # Добавляем сообщение пользователя в историю\n",
    "    chat_history.append(AIMessage(content=response[\"output\"]))  # Добавляем ответ агента в историю"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Самый быстрый и в целом четкийй вариант"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начните общение с ИИ! Введите ‘выход’, чтобы завершить разговор.\n",
      "Вы: что ты умеешь?\n",
      "AI: Я могу отвечать на вопросы о компании Intelion, включая информацию о ее деятельности, задачах и стратегиях. Если у вас есть конкретный вопрос, задайте его, и я постараюсь помочь.\n"
     ]
    }
   ],
   "source": [
    "# Инициализируем retriever с типом поиска по схожести, чтобы получать k=3 наиболее релевантных результатов\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",  # Используем метод поиска по схожести\n",
    "    search_kwargs={\"k\": 5},  # Количество возвращаемых результатов: 3\n",
    ")\n",
    "\n",
    "# Создаем модель ChatOpenAI с использованием модели GPT-4o\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)  # Модель GPT-4o для обработки запросов\n",
    "\n",
    "# Определяем системный промпт для контекстуализации вопросов\n",
    "# Этот промпт объясняет ИИ, как следует формулировать вопрос на основе истории чата\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Учитывая историю чата и последний вопрос пользователя, \"  # Описание задачи\n",
    "    \"который может ссылаться на контекст из истории чата, \"  # Учитывание истории чата\n",
    "    \"сформулируйте самодостаточный вопрос, который можно понять \"  # Требование сделать вопрос самодостаточным\n",
    "    \"без использования истории чата. НЕ отвечайте на вопрос, просто \"  # Не нужно отвечать, только переформулировать вопрос\n",
    "    \"переформулируйте его, если это необходимо, или верните его без изменений.\"\n",
    "# )\n",
    "    # \"Вы являетесь ассистентом по языковой модели искусственного интеллекта.\"\n",
    "    # \"Ваша задача - сгенерировать 3 различных подвопросов Или альтернативных версий данного пользовательского вопроса для извлечения соответствующих документов из векторной базы данных.\"\n",
    "    # \"Сгенерировав несколько версий пользовательского вопроса, ваша цель - помочь пользователю преодолеть некоторые ограничения поиска сходства на основе расстояния.\"\n",
    "    # \"Создавая дополнительные вопросы, вы можете разбить вопросы, относящиеся к нескольким концепциям, на отдельные вопросы.\"\n",
    "    # \"Это поможет вам получить соответствующие документы для составления окончательного ответа\"\n",
    "    # \"Если в вопросе присутствует несколько концепций, вам следует разбить его на подзадачи, задав по одному вопросу для каждой концепции\"\n",
    "    )\n",
    "\n",
    "# Создаем шаблон для формирования контекстуализированных вопросов\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),  # Системное сообщение с инструкцией для ИИ\n",
    "        MessagesPlaceholder(\"chat_history\"),  # Место для истории чата\n",
    "        (\"human\", \"{input}\"),  # Ввод пользователя, который нужно переформулировать\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Создаем retriever, учитывающий историю чата\n",
    "# Этот retriever использует LLM для помощи в переформулировании вопросов с учетом контекста\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt  # Используем созданный шаблон и retriever для поиска с историей\n",
    ")\n",
    "\n",
    "# Определяем системный промпт для ответа на вопросы\n",
    "# Этот промпт объясняет ИИ, как отвечать на вопросы на основе контекста и что делать, если ответа нет\n",
    "qa_system_prompt = (\n",
    "    \"Вы помощник для задач по ответам на вопросы о компании Intelion. Используйте \"  # Инструкция для ИИ\n",
    "    \"следующие части полученного контекста, чтобы ответить на \"  # Использование контекста для ответа\n",
    "    \"вопрос. Если вы не знаете ответа, просто скажите, что вы \"  # Если ответа нет, сообщить об этом\n",
    "    \"не знаете. Используйте максимум три предложения и дайте \"  # Ответ должен быть кратким\n",
    "    \"краткий ответ.\"  # Старайтесь отвечать как можно более сжато\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"  # Контекст, который будет использован для ответа\n",
    ")\n",
    "\n",
    "# Создаем шаблон для формирования ответов на вопросы\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),  # Системное сообщение с инструкцией для ИИ\n",
    "        MessagesPlaceholder(\"chat_history\"),  # Место для истории чата\n",
    "        (\"human\", \"{input}\"),  # Ввод пользователя, на который нужно ответить\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Создаем цепочку для объединения документов для ответов на вопросы\n",
    "# `create_stuff_documents_chain` передает весь найденный контекст в LLM для обработки\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "# Создаем retrieval-цепочку, которая объединяет retriever с историей и цепочку для ответов на вопросы\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "\n",
    "# Функция для моделирования непрерывного чата\n",
    "def continual_chat():\n",
    "    print(\"Начните общение с ИИ! Введите ‘выход’, чтобы завершить разговор.\")  # Начало общения с ИИ\n",
    "    chat_history = []  # Хранение истории чата\n",
    "    while True:\n",
    "        query = input(\"ВЫ: \")  # Получение запроса от пользователя\n",
    "        if query.lower() == \"выход\":  # Если пользователь хочет завершить чат\n",
    "            break\n",
    "        # Обработка запроса пользователя через цепочку retrieval\n",
    "        result = rag_chain.invoke({\"input\": query, \"chat_history\": chat_history})  # Запрос к цепочке\n",
    "        # Печать вопроса пользователя и ответа ИИ\n",
    "        print(f\"Вы: {query}\")  # Печать вопроса пользователя\n",
    "        print(f\"AI: {result['answer']}\")  # Ответ ИИ\n",
    "        # Обновление истории чата\n",
    "        chat_history.append(HumanMessage(content=query))  # Добавление сообщения пользователя в историю\n",
    "        chat_history.append(SystemMessage(content=result[\"answer\"]))  # Добавление ответа ИИ в историю\n",
    "\n",
    "# Main function to start the continual chat\n",
    "if __name__ == \"__main__\":\n",
    "    continual_chat()\n",
    "\n",
    "\n",
    "# # Функция для моделирования непрерывного чата с выводом переформулированного вопроса\n",
    "# def continual_chat():\n",
    "#     print(\"Начните общение с ИИ! Введите ‘выход’, чтобы завершить разговор.\")  # Начало общения с ИИ\n",
    "#     chat_history = []  # Хранение истории чата\n",
    "#     while True:\n",
    "#         query = input(\"ВЫ: \")  # Получение запроса от пользователя\n",
    "#         if query.lower() == \"выход\":  # Если пользователь хочет завершить чат\n",
    "#             break\n",
    "\n",
    "#         # Переформулировка вопроса через вызов LLM с контекстом\n",
    "#         reformulated_prompt = contextualize_q_prompt.format_messages(\n",
    "#             chat_history=chat_history, input=query\n",
    "#         )\n",
    "#         reformulated_query_result = llm(reformulated_prompt)  # Получаем объект AIMessage\n",
    "#         reformulated_query = reformulated_query_result.content  # Доступ к содержимому через атрибут content\n",
    "        \n",
    "#         print(f\"Переформулированный вопрос: {reformulated_query}\")  # Вывод переформулированного вопроса\n",
    "        \n",
    "#         # Обработка запроса пользователя через цепочку retrieval\n",
    "#         result = rag_chain.invoke({\"input\": reformulated_query, \"chat_history\": chat_history})  # Запрос к цепочке\n",
    "        \n",
    "#         # Печать вопроса пользователя и ответа ИИ\n",
    "#         print(f\"Вы: {query}\")  # Печать вопроса пользователя\n",
    "#         print(f\"AI: {result['answer']}\")  # Ответ ИИ\n",
    "        \n",
    "#         # Обновление истории чата\n",
    "#         chat_history.append(HumanMessage(content=query))  # Добавление сообщения пользователя в историю\n",
    "#         chat_history.append(SystemMessage(content=result[\"answer\"]))  # Добавление ответа ИИ в историю\n",
    "\n",
    "# # Main function to start the continual chat\n",
    "# if __name__ == \"__main__\":\n",
    "#     continual_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Поиск документов с использованием RAG-Fusion и RRF для поиска и ранжирования документов:\n",
    "## После переформулировки вопрос передается в генерацию нескольких запросов, и система выполняет поиск документов для каждого из них."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Этот вариант работает медленнее и не сказал бы что ответы лучше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начните общение с ИИ помошником о компании Intellion! Введите ‘выход’, чтобы завершить разговор.\n",
      "Вы: Как ты можешь помочь?\n",
      "AI: Я могу ответить на вопросы о компании Intelion и ее деятельности, а также предоставить информацию о людях, связанных с ней, если такая информация есть. Если у вас есть конкретный вопрос, задайте его, и я постараюсь помочь!\n",
      "Вы: Можешь рассказать историю компании в 5 пунктах?\n",
      "AI: 1. Компания Intelion была основана в 2017 году и изначально предоставляла услуги, связанные с вычислительными технологиями.\n",
      "2. В 2023 году Intelion стала соучредителем инициативы по регулированию отрасли.\n",
      "3. Компания зарекомендовала себя как промышленный оператор по продаже и обслуживанию вычислительного оборудования.\n",
      "4. За последние 5 лет выручка Intelion увеличилась почти в 40 раз, что свидетельствует о значительном росте.\n",
      "5. Для поддержания высоких темпов роста Intelion привлекает частные инвестиции в два направления.\n",
      "Вы: А кто ген директор компании?\n",
      "AI: Я не знаю, кто является генеральным директором компании Intelion.\n",
      "Вы: Что такое ПФП?\n",
      "AI: Персональный финансовый план (ПФП) – это наиболее точный инструмент персонального планирования, который помогает в управлении финансами и достижении финансовых целей. Он включает в себя анализ текущего финансового состояния и разработку стратегии для достижения желаемых результатов.\n",
      "Вы: А что вы можете предложить если я хочу инвестировать в майнинг?\n",
      "AI: Мы предлагаем комплексные решения по инвестициям в майнинг и майнинговое оборудование. Рекомендуем диверсифицировать портфель и не вкладывать все свободные деньги в одну область. Также мы знакомим клиентов с персональными услугами и предлагаем информацию о потенциальной доходности, которая может достигать 40-80% годовых.\n",
      "Вы: А есть какие то комплексные решения?\n",
      "AI: Да, компания Intelion предлагает комплексные решения по инвестициям в майнинг, включая подбор оптимального инвестиционного оборудования и услуги по его обслуживанию. Также они обеспечивают бесперебойное электроснабжение и минимизируют операционные нагрузки для инвесторов. Кроме того, Intelion строит собственные центры обработки данных (ЦОДы) для поддержки майнинговых операций.\n",
      "Вы: у компании есть регламент по отпускам?\n",
      "AI: Да, у компании Intelion есть регламент по отпускам. Он включает информацию о праве на отпуск после полугода работы, а также порядок оформления и подачи заявления на отпуск. Сотрудник имеет право на 3 нерабочих дня в течение года после указанного срока.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.load import dumps, loads\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Инициализируем retriever с типом поиска по схожести, чтобы получать k=5 наиболее релевантных результатов\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",  # Используем метод поиска по схожести\n",
    "    search_kwargs={\"k\": 5},  # Количество возвращаемых результатов\n",
    ")\n",
    "\n",
    "# Создаем модель ChatOpenAI с использованием модели GPT-4o\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")  # Модель GPT-4o для обработки запросов\n",
    "\n",
    "# Определяем системный промпт для контекстуализации вопросов\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Учитывая историю чата и последний вопрос пользователя, \"\n",
    "    \"который может ссылаться на контекст из истории чата, \"\n",
    "    \"сформулируйте самодостаточный вопрос, который можно понять \"\n",
    "    \"без использования истории чата. НЕ отвечайте на вопрос, просто \"\n",
    "    \"переформулируйте его, если это необходимо, или верните его без изменений.\"\n",
    ")\n",
    "\n",
    "# Создаем шаблон для формирования контекстуализированных вопросов\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),  # Системное сообщение с инструкцией для ИИ\n",
    "        MessagesPlaceholder(\"chat_history\"),  # Место для истории чата\n",
    "        (\"human\", \"{input}\"),  # Ввод пользователя, который нужно переформулировать\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Шаблон для генерации нескольких запросов\n",
    "template = \"\"\"Вы являетесь полезным помощником о компании Intellion, который генерирует несколько поисковых запросов на основе одного входного запроса. \\n\n",
    "Сгенерируйте несколько поисковых запросов, связанных с: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Функция для генерации нескольких запросов\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | ChatOpenAI(temperature=0) \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "# Функция для Reciprocal Rank Fusion\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\"Функция RRF для слияния результатов поиска из нескольких списков.\"\"\"\n",
    "    fused_scores = {}\n",
    "    for docs in results:\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "    \n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "    \n",
    "    return reranked_results\n",
    "\n",
    "# Функция для создания retrieval-цепочки с использованием RAG и RRF\n",
    "def create_rag_fusion_retrieval_chain(retriever, question):\n",
    "    # Генерируем несколько запросов\n",
    "    generated_queries = generate_queries.invoke({\"question\": question})\n",
    "    \n",
    "    # Выполняем поиск по базе данных для каждого запроса\n",
    "    search_results = [retriever.get_relevant_documents(query) for query in generated_queries]\n",
    "    \n",
    "    # Применяем RRF для слияния результатов\n",
    "    reranked_docs = reciprocal_rank_fusion(search_results)\n",
    "    \n",
    "    # Извлекаем только документы, без их рангов\n",
    "    return [doc for doc, score in reranked_docs]\n",
    "\n",
    "# Создаем шаблон для ответа на вопросы на основе контекста\n",
    "qa_system_prompt = (\n",
    "    \"Вы помощник для задач по ответам на вопросы о компании Intellion. Используйте \"\n",
    "    \"следующие части полученного контекста, чтобы ответить на \"\n",
    "    \"вопрос. Если вы не знаете ответа, просто скажите, что вы \"\n",
    "    \"не знаете. Используйте максимум три предложения и дайте \"\n",
    "    \"краткий ответ.\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Функция для обработки запроса с контекстуализацией и RAG\n",
    "def process_query(query, chat_history):\n",
    "    # Шаг 1: Переформулируем вопрос с учетом истории чата\n",
    "    reformulated_prompt = contextualize_q_prompt.format_messages(\n",
    "        chat_history=chat_history, input=query\n",
    "    )\n",
    "    reformulated_query_result = llm(reformulated_prompt)  # Получаем объект AIMessage\n",
    "    reformulated_query = reformulated_query_result.content  # Доступ к содержимому\n",
    "    \n",
    "    # Шаг 2: Получаем документы с использованием RAG-Fusion и RRF\n",
    "    docs = create_rag_fusion_retrieval_chain(retriever, reformulated_query)\n",
    "    \n",
    "    # Шаг 3: Генерируем ответ на основе контекста\n",
    "    result = question_answer_chain.invoke({\"input\": query, \"context\": docs, \"chat_history\": chat_history})\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Функция для моделирования непрерывного чата\n",
    "def continual_chat():\n",
    "    print(\"Начните общение с ИИ помошником о компании Intellion! Введите ‘выход’, чтобы завершить разговор.\")\n",
    "    chat_history = []  # Хранение истории чата\n",
    "    while True:\n",
    "        query = input(\"ВЫ: \")  # Получение запроса от пользователя\n",
    "        if query.lower() == \"выход\":  # Если пользователь хочет завершить чат\n",
    "            break\n",
    "        \n",
    "        # Обрабатываем запрос пользователя через систему RAG с RRF\n",
    "        result = process_query(query, chat_history)\n",
    "        \n",
    "        # Печать вопроса пользователя и ответа ИИ\n",
    "        print(f\"Вы: {query}\")\n",
    "        print(f\"AI: {result}\")  # Выводим результат, так как это строка\n",
    "        \n",
    "        # Обновление истории чата\n",
    "        chat_history.append(HumanMessage(content=query))\n",
    "        chat_history.append(SystemMessage(content=result))\n",
    "\n",
    "# Запуск непрерывного чата\n",
    "if __name__ == \"__main__\":\n",
    "    continual_chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
